{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.1.1-cp39-none-macosx_11_0_arm64.whl (59.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 59.6 MB 36.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.16.1-cp39-cp39-macosx_11_0_arm64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 45.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading torchaudio-2.1.1-cp39-cp39-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Using cached fsspec-2023.12.0-py3-none-any.whl (168 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 44.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 49.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from torch) (4.8.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-10.1.0-cp39-cp39-macosx_11_0_arm64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 57.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.26.2-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.0 MB 57.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.3-cp39-cp39-macosx_10_9_universal2.whl (17 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 33.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 36.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 625 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 58.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: mpmath, MarkupSafe, urllib3, sympy, networkx, jinja2, idna, fsspec, filelock, charset-normalizer, certifi, torch, requests, pillow, numpy, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.3 certifi-2023.11.17 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2023.12.0 idna-3.6 jinja2-3.1.2 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.2 pillow-10.1.0 requests-2.31.0 sympy-1.12 torch-2.1.1 torchaudio-2.1.1 torchvision-0.16.1 urllib3-2.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp39-cp39-macosx_11_0_arm64.whl (426 kB)\n",
      "\u001b[K     |████████████████████████████████| 426 kB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (3.13.1)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: requests in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (2.31.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Using cached huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (1.26.2)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.10.3-cp39-cp39-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[K     |████████████████████████████████| 291 kB 40.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[K     |████████████████████████████████| 174 kB 37.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 40.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.1.0)\n",
      "Installing collected packages: tqdm, pyyaml, huggingface-hub, tokenizers, safetensors, regex, transformers\n",
      "Successfully installed huggingface-hub-0.19.4 pyyaml-6.0.1 regex-2023.10.3 safetensors-0.4.1 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.3.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.35.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages\n",
      "Requires: tqdm, regex, tokenizers, pyyaml, numpy, safetensors, packaging, filelock, huggingface-hub, requests\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting s3fs\n",
      "  Using cached s3fs-2023.12.0-py3-none-any.whl (29 kB)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4\n",
      "  Using cached aiobotocore-2.8.0-py3-none-any.whl (75 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.9.1-cp39-cp39-macosx_11_0_arm64.whl (387 kB)\n",
      "\u001b[K     |████████████████████████████████| 387 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec==2023.12.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from s3fs) (2023.12.0)\n",
      "Collecting wrapt<2.0.0,>=1.10.10\n",
      "  Downloading wrapt-1.16.0-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting botocore<1.33.2,>=1.32.4\n",
      "  Using cached botocore-1.33.1-py3-none-any.whl (11.6 MB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp39-cp39-macosx_11_0_arm64.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.3-cp39-cp39-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 26.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attrs>=17.3.0\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 17.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from aioitertools<1.0.0,>=0.5.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from botocore<1.33.2,>=1.32.4->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.8.2)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[K     |████████████████████████████████| 143 kB 38.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.33.2,>=1.32.4->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.15.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.6)\n",
      "Installing collected packages: multidict, frozenlist, yarl, urllib3, jmespath, attrs, async-timeout, aiosignal, wrapt, botocore, aioitertools, aiohttp, aiobotocore, s3fs\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "Successfully installed aiobotocore-2.8.0 aiohttp-3.9.1 aioitertools-0.11.0 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 botocore-1.33.1 frozenlist-1.4.0 jmespath-1.0.1 multidict-6.0.4 s3fs-2023.12.0 urllib3-1.26.18 wrapt-1.16.0 yarl-1.9.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fsspec==2023.3.0\n",
      "  Using cached fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "Installing collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.12.0\n",
      "    Uninstalling fsspec-2023.12.0:\n",
      "      Successfully uninstalled fsspec-2023.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.12.0 requires fsspec==2023.12.0, but you have fsspec 2023.3.0 which is incompatible.\n",
      "huggingface-hub 0.19.4 requires fsspec>=2023.5.0, but you have fsspec 2023.3.0 which is incompatible.\u001b[0m\n",
      "Successfully installed fsspec-2023.3.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fsspec==2023.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from nltk) (2023.10.3)\n",
      "Collecting click\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: tqdm in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from nltk) (4.66.1)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 44.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.1.3-cp39-cp39-macosx_11_0_arm64.whl (11.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.0 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[K     |████████████████████████████████| 341 kB 40.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[K     |████████████████████████████████| 502 kB 52.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.22.4 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.1.3 pytz-2023.3.post1 tzdata-2023.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (1.26.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data = pd.read_csv('master_anime_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset of your data for processing\n",
    "sampled_data = master_data.sample(frac=0.1, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kariprimiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synopsis</th>\n",
       "      <th>emotion_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19101</th>\n",
       "      <td>After the accident in which she lost her mothe...</td>\n",
       "      <td>{'anger': 0.010413373820483685, 'disgust': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217920</th>\n",
       "      <td>Naota Nandaba is an ordinary sixth grader livi...</td>\n",
       "      <td>{'anger': 0.18671022355556488, 'disgust': 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267563</th>\n",
       "      <td>Story begins as Momiji's chastity is taken by ...</td>\n",
       "      <td>{'anger': 0.007550527341663837, 'disgust': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>Tanya Degurechaff is a young soldier infamous ...</td>\n",
       "      <td>{'anger': 0.5558895468711853, 'disgust': 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165782</th>\n",
       "      <td>Hachiman Hikigaya is an apathetic high school ...</td>\n",
       "      <td>{'anger': 0.011294235475361347, 'disgust': 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 synopsis  \\\n",
       "19101   After the accident in which she lost her mothe...   \n",
       "217920  Naota Nandaba is an ordinary sixth grader livi...   \n",
       "267563  Story begins as Momiji's chastity is taken by ...   \n",
       "9909    Tanya Degurechaff is a young soldier infamous ...   \n",
       "165782  Hachiman Hikigaya is an apathetic high school ...   \n",
       "\n",
       "                                           emotion_scores  \n",
       "19101   {'anger': 0.010413373820483685, 'disgust': 0.0...  \n",
       "217920  {'anger': 0.18671022355556488, 'disgust': 0.00...  \n",
       "267563  {'anger': 0.007550527341663837, 'disgust': 0.0...  \n",
       "9909    {'anger': 0.5558895468711853, 'disgust': 0.003...  \n",
       "165782  {'anger': 0.011294235475361347, 'disgust': 0.0...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the emotion classification pipeline\n",
    "model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "emotion_classifier = pipeline('text-classification', model=model_name, return_all_scores=True)\n",
    "\n",
    "# Set English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for advanced text cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stop words\n",
    "    return text\n",
    "\n",
    "# Function to classify emotions of a text\n",
    "def classify_emotions(text):\n",
    "    cleaned_text = clean_text(text)  # Clean the text using clean_text function\n",
    "    predictions = emotion_classifier(cleaned_text)\n",
    "    return {emotion['label']: emotion['score'] for emotion in predictions[0]}\n",
    "\n",
    "# Apply the model to the 'synopsis' column\n",
    "sampled_data['emotion_scores'] = sampled_data['synopsis'].astype(str).apply(classify_emotions)\n",
    "\n",
    "# Inspect results\n",
    "sampled_data[['synopsis', 'emotion_scores']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 19101     {'anger': 0.010413373820483685, 'disgust': 0.0...\n",
       "217920    {'anger': 0.18671022355556488, 'disgust': 0.00...\n",
       "267563    {'anger': 0.007550527341663837, 'disgust': 0.0...\n",
       "9909      {'anger': 0.5558895468711853, 'disgust': 0.003...\n",
       "165782    {'anger': 0.011294235475361347, 'disgust': 0.0...\n",
       "                                ...                        \n",
       "182181    {'anger': 0.0200864989310503, 'disgust': 0.000...\n",
       "57783     {'anger': 0.007323819678276777, 'disgust': 0.0...\n",
       "154919    {'anger': 0.005832878407090902, 'disgust': 0.0...\n",
       "265514    {'anger': 0.004111292771995068, 'disgust': 0.0...\n",
       "85081     {'anger': 0.022216347977519035, 'disgust': 0.0...\n",
       "Name: emotion_scores, Length: 34988, dtype: object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data['emotion_scores'].value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding 'emotion_scores' into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid_review</th>\n",
       "      <th>profile</th>\n",
       "      <th>anime_uid</th>\n",
       "      <th>scores</th>\n",
       "      <th>Overall</th>\n",
       "      <th>Story</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Sound</th>\n",
       "      <th>Character</th>\n",
       "      <th>Enjoyment</th>\n",
       "      <th>...</th>\n",
       "      <th>favorites_anime</th>\n",
       "      <th>link_y</th>\n",
       "      <th>emotion_scores</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>neutral</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19101</th>\n",
       "      <td>27270</td>\n",
       "      <td>Elfsire</td>\n",
       "      <td>120</td>\n",
       "      <td>{'Overall': '8', 'Story': '8', 'Animation': '1...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>['14713', '14289', '4081', '14467', '16498', '...</td>\n",
       "      <td>https://myanimelist.net/profile/Elfsire</td>\n",
       "      <td>{'anger': 0.010413373820483685, 'disgust': 0.0...</td>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.197999</td>\n",
       "      <td>0.028194</td>\n",
       "      <td>0.013339</td>\n",
       "      <td>0.689401</td>\n",
       "      <td>0.059908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217920</th>\n",
       "      <td>228044</td>\n",
       "      <td>merryfistmas</td>\n",
       "      <td>227</td>\n",
       "      <td>{'Overall': '10', 'Story': '10', 'Animation': ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>['18679', '22135', '227', '23847', '5114', '11...</td>\n",
       "      <td>https://myanimelist.net/profile/merryfistmas</td>\n",
       "      <td>{'anger': 0.18671022355556488, 'disgust': 0.00...</td>\n",
       "      <td>0.186710</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.069133</td>\n",
       "      <td>0.091217</td>\n",
       "      <td>0.139144</td>\n",
       "      <td>0.045635</td>\n",
       "      <td>0.465496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267563</th>\n",
       "      <td>21744</td>\n",
       "      <td>ravenrage</td>\n",
       "      <td>3303</td>\n",
       "      <td>{'Overall': '6', 'Story': '3', 'Animation': '8...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>['134']</td>\n",
       "      <td>https://myanimelist.net/profile/ravenrage</td>\n",
       "      <td>{'anger': 0.007550527341663837, 'disgust': 0.0...</td>\n",
       "      <td>0.007551</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.658758</td>\n",
       "      <td>0.077451</td>\n",
       "      <td>0.017352</td>\n",
       "      <td>0.235827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>248039</td>\n",
       "      <td>Tyrannicswine117</td>\n",
       "      <td>32615</td>\n",
       "      <td>{'Overall': '8', 'Story': '9', 'Animation': '8...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>['5114', '4181', '9253', '18679', '1', '2001',...</td>\n",
       "      <td>https://myanimelist.net/profile/Tyrannicswine117</td>\n",
       "      <td>{'anger': 0.5558895468711853, 'disgust': 0.003...</td>\n",
       "      <td>0.555890</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.173663</td>\n",
       "      <td>0.078683</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>0.014780</td>\n",
       "      <td>0.082877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165782</th>\n",
       "      <td>215510</td>\n",
       "      <td>ThePunzalan</td>\n",
       "      <td>14813</td>\n",
       "      <td>{'Overall': '9', 'Story': '9', 'Animation': '9...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>['14813', '30831', '15809', '4224', '31715', '...</td>\n",
       "      <td>https://myanimelist.net/profile/ThePunzalan</td>\n",
       "      <td>{'anger': 0.011294235475361347, 'disgust': 0.0...</td>\n",
       "      <td>0.011294</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.938779</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>0.031252</td>\n",
       "      <td>0.009808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid_review           profile  anime_uid  \\\n",
       "19101        27270           Elfsire        120   \n",
       "217920      228044      merryfistmas        227   \n",
       "267563       21744         ravenrage       3303   \n",
       "9909        248039  Tyrannicswine117      32615   \n",
       "165782      215510       ThePunzalan      14813   \n",
       "\n",
       "                                                   scores  Overall  Story  \\\n",
       "19101   {'Overall': '8', 'Story': '8', 'Animation': '1...        8      8   \n",
       "217920  {'Overall': '10', 'Story': '10', 'Animation': ...       10     10   \n",
       "267563  {'Overall': '6', 'Story': '3', 'Animation': '8...        6      3   \n",
       "9909    {'Overall': '8', 'Story': '9', 'Animation': '8...        8      9   \n",
       "165782  {'Overall': '9', 'Story': '9', 'Animation': '9...        9      9   \n",
       "\n",
       "        Animation  Sound  Character  Enjoyment  ...  \\\n",
       "19101          10      8         10          7  ...   \n",
       "217920         10     10         10         10  ...   \n",
       "267563          8      5          6          7  ...   \n",
       "9909            8      8          9          8  ...   \n",
       "165782          9      9         10          9  ...   \n",
       "\n",
       "                                          favorites_anime  \\\n",
       "19101   ['14713', '14289', '4081', '14467', '16498', '...   \n",
       "217920  ['18679', '22135', '227', '23847', '5114', '11...   \n",
       "267563                                            ['134']   \n",
       "9909    ['5114', '4181', '9253', '18679', '1', '2001',...   \n",
       "165782  ['14813', '30831', '15809', '4224', '31715', '...   \n",
       "\n",
       "                                                  link_y  \\\n",
       "19101            https://myanimelist.net/profile/Elfsire   \n",
       "217920      https://myanimelist.net/profile/merryfistmas   \n",
       "267563         https://myanimelist.net/profile/ravenrage   \n",
       "9909    https://myanimelist.net/profile/Tyrannicswine117   \n",
       "165782       https://myanimelist.net/profile/ThePunzalan   \n",
       "\n",
       "                                           emotion_scores     anger   disgust  \\\n",
       "19101   {'anger': 0.010413373820483685, 'disgust': 0.0...  0.010413  0.000746   \n",
       "217920  {'anger': 0.18671022355556488, 'disgust': 0.00...  0.186710  0.002664   \n",
       "267563  {'anger': 0.007550527341663837, 'disgust': 0.0...  0.007551  0.001111   \n",
       "9909    {'anger': 0.5558895468711853, 'disgust': 0.003...  0.555890  0.003051   \n",
       "165782  {'anger': 0.011294235475361347, 'disgust': 0.0...  0.011294  0.000683   \n",
       "\n",
       "            fear       joy   neutral   sadness  surprise  \n",
       "19101   0.197999  0.028194  0.013339  0.689401  0.059908  \n",
       "217920  0.069133  0.091217  0.139144  0.045635  0.465496  \n",
       "267563  0.001951  0.658758  0.077451  0.017352  0.235827  \n",
       "9909    0.173663  0.078683  0.091056  0.014780  0.082877  \n",
       "165782  0.000702  0.938779  0.007481  0.031252  0.009808  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the emotion_scores column needs to be parsed from string to dictionary\n",
    "if isinstance(sampled_data['emotion_scores'].iloc[0], str):\n",
    "    sampled_data['emotion_scores'] = sampled_data['emotion_scores'].apply(ast.literal_eval)\n",
    "\n",
    "# Expand the emotion_scores column into separate columns\n",
    "emotion_scores = sampled_data['emotion_scores'].apply(pd.Series)\n",
    "\n",
    "# Join the expanded emotion scores dataframe with the original data\n",
    "data_with_emotions = sampled_data.join(emotion_scores)\n",
    "\n",
    "# Display the first few rows to verify the expansion\n",
    "data_with_emotions.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model for Mood Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the mood categories based on the highest emotion score\n",
    "mood_categories = data_with_emotions[['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']].idxmax(axis=1)\n",
    "\n",
    "# Add the mood categories to the DataFrame\n",
    "data_with_emotions['mood_category'] = mood_categories\n",
    "\n",
    "# Prepare the data\n",
    "X = data_with_emotions[['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']]  # Features\n",
    "y = data_with_emotions['mood_category']  # Labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-Based Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mood_label(row):\n",
    "    if row['joy'] > 0.5:\n",
    "        return 'Happy'\n",
    "    elif row['sadness'] > 0.5:\n",
    "        return 'Sad'\n",
    "    elif row['anger'] > 0.5:\n",
    "        return 'Angry'\n",
    "    # Add more rules as needed\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "data_with_emotions['mood_label'] = data_with_emotions.apply(create_mood_label, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustered Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Assuming 'features' is the matrix of features we're using for clustering\n",
    "features = data_with_emotions[['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']].values\n",
    "\n",
    "# We'll test silhouette scores for different numbers of clusters\n",
    "range_n_clusters = list(range(2, 11))  # For example, testing 2 to 10 clusters\n",
    "silhouette_avg_scores = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = clusterer.fit_predict(features)\n",
    "    silhouette_avg = silhouette_score(features, cluster_labels)\n",
    "    silhouette_avg_scores.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {n_clusters}, the average silhouette_score is : {silhouette_avg}\")\n",
    "\n",
    "# Plot the silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range_n_clusters, silhouette_avg_scores, marker='o')\n",
    "plt.title('Silhouette Analysis For Optimal k')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming data_with_emotions is your DataFrame with the emotion scores as separate columns\n",
    "# We will use these scores as features for clustering\n",
    "features = data_with_emotions[['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']].values\n",
    "\n",
    "# Choose the number of clusters (k) you want to divide your data into\n",
    "# The choice of k could be informed by domain knowledge, or you could use methods like the elbow method to choose k\n",
    "k = 3\n",
    "\n",
    "# Perform K-Means clustering\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "data_with_emotions['cluster'] = kmeans.fit_predict(features)\n",
    "\n",
    "# Now let's see the clusters\n",
    "# Since we have multiple dimensions (each emotion score is a dimension), let's reduce it to two dimensions using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(features)\n",
    "\n",
    "# Plot the reduced features in a scatter plot and color them with their cluster label\n",
    "plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=data_with_emotions['cluster'], cmap='viridis')\n",
    "plt.xlabel('PCA Feature 1')\n",
    "plt.ylabel('PCA Feature 2')\n",
    "plt.title('Anime Emotion Scores Clustering')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "\n",
    "# Show the centroids of each cluster in the reduced feature space\n",
    "centroids = pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200, color='r')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Super-users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for super-users (top 5%): 56.00\n",
      "Identified 970 super-users.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of interactions per user\n",
    "interaction_counts = master_data.groupby('profile').size()\n",
    "\n",
    "# Using percentiles to define super-users\n",
    "percentile_threshold = 95  # Top 5% of users\n",
    "threshold = interaction_counts.quantile(percentile_threshold / 100)\n",
    "super_users = interaction_counts[interaction_counts > threshold].index.tolist()\n",
    "\n",
    "print(f\"Threshold for super-users (top {100 - percentile_threshold}%): {threshold:.2f}\")\n",
    "print(f\"Identified {len(super_users)} super-users.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data_with_emotions subset - approx. 34k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for super-users (top 5%): 10.00\n",
      "Identified 497 super-users.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of interactions per user\n",
    "interaction_counts = data_with_emotions.groupby('profile').size()\n",
    "\n",
    "# Using percentiles to define super-users\n",
    "percentile_threshold = 95  # Top 5% of users\n",
    "threshold = interaction_counts.quantile(percentile_threshold / 100)\n",
    "super_users = interaction_counts[interaction_counts > threshold].index.tolist()\n",
    "\n",
    "print(f\"Threshold for super-users (top {100 - percentile_threshold}%): {threshold:.2f}\")\n",
    "print(f\"Identified {len(super_users)} super-users.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New dataset with and without super-users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#super_users = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering-Based Hybrid Methods\n",
    "Perform clustering on user and item features and then apply collaborative and content-based methods within each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "Combine multiple different recommendation models using ensemble techniques. For example, using a voting or stacking approach where predictions from multiple models are combined to produce the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Collaborative Filtering (NCF)\n",
    "NCF models use neural networks to learn user-item interaction patterns, providing a more flexible and powerful way to model complex relationships compared to traditional matrix factorization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp39-cp39-macosx_12_0_arm64.whl (2.1 kB)\n",
      "Collecting tensorflow-macos==2.15.0\n",
      "  Downloading tensorflow_macos-2.15.0-cp39-cp39-macosx_12_0_arm64.whl (208.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 208.8 MB 8.1 kB/s  eta 0:00:01████████████████▍             | 119.6 MB 79.2 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[K     |████████████████████████████████| 394 kB 72.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=23.5.26\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-macosx_10_9_universal2.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 44.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.15.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.26.2)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp39-cp39-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 39.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.10.0-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 80.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-macosx_11_0_arm64.whl (35 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow-macos==2.15.0->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: packaging in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.2)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl (20.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.6 MB 46.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.59.3-cp39-cp39-macosx_10_10_universal2.whl (9.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.6 MB 66.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<2.16,>=2.15\n",
      "  Using cached tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.8.0)\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.37.0)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "\u001b[K     |████████████████████████████████| 102 kB 33.2 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[K     |████████████████████████████████| 226 kB 54.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.24.0-py2.py3-none-any.whl (183 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 75.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.17.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.6)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.3)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, ml-dtypes, libclang, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow-macos, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.24.0 google-auth-oauthlib-1.1.0 google-pasta-0.2.0 grpcio-1.59.3 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 markdown-3.5.1 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.34.0 tensorflow-macos-2.15.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.14.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (2.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Input, concatenate, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "# Columns: 'uid_review' for users, 'anime_uid' for items, and 'Overall' for ratings\n",
    "# Convert user and item IDs to categorical variables\n",
    "data_with_emotions['uid_review'] = data_with_emotions['uid_review'].astype('category')\n",
    "data_with_emotions['anime_uid'] = data_with_emotions['anime_uid'].astype('category')\n",
    "\n",
    "# Map user and item IDs to integers\n",
    "user_id_mapping = {id: i for i, id in enumerate(data_with_emotions['uid_review'].cat.categories)}\n",
    "item_id_mapping = {id: i for i, id in enumerate(data_with_emotions['anime_uid'].cat.categories)}\n",
    "\n",
    "# Apply mapping\n",
    "data_with_emotions['user'] = data_with_emotions['uid_review'].map(user_id_mapping)\n",
    "data_with_emotions['item'] = data_with_emotions['anime_uid'].map(item_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "394/394 [==============================] - 1s 1ms/step - loss: 22.6378 - val_loss: 4.1832\n",
      "Epoch 2/5\n",
      "394/394 [==============================] - 1s 1ms/step - loss: 2.2857 - val_loss: 2.8225\n",
      "Epoch 3/5\n",
      "394/394 [==============================] - 1s 1ms/step - loss: 0.6315 - val_loss: 2.7547\n",
      "Epoch 4/5\n",
      "394/394 [==============================] - 1s 1ms/step - loss: 0.3145 - val_loss: 2.7346\n",
      "Epoch 5/5\n",
      "394/394 [==============================] - 1s 1ms/step - loss: 0.2194 - val_loss: 2.5592\n",
      "219/219 [==============================] - 0s 289us/step - loss: 2.6124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.6123688220977783"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data for the model\n",
    "X = data_with_emotions[['user', 'item']].values\n",
    "y = data_with_emotions['Overall'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "# Neural Collaborative Filtering model architecture\n",
    "user_input = Input(shape=(1,))\n",
    "item_input = Input(shape=(1,))\n",
    "user_embedding = Embedding(len(user_id_mapping), 15, input_length=1)(user_input)\n",
    "item_embedding = Embedding(len(item_id_mapping), 15, input_length=1)(item_input)\n",
    "user_vec = Flatten()(user_embedding)\n",
    "item_vec = Flatten()(item_embedding)\n",
    "concat = concatenate([user_vec, item_vec])\n",
    "dense = Dense(64, activation='relu')(concat)\n",
    "output = Dense(1)(dense)\n",
    "model = Model([user_input, item_input], output)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "model.fit([X_train[:, 0], X_train[:, 1]], y_train, batch_size=64, epochs=5, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate([X_test[:, 0], X_test[:, 1]], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a Neural Collaborative Filtering (NCF) model, which is designed to predict user ratings for items (anime) based on learned user and item embeddings.\n",
    "\n",
    "### Model Architecture:\n",
    "\n",
    "- Model has separate embedding layers for users (user_embedding) and items (item_embedding), each with an embedding size of 15. This means the model learns a 15-dimensional vector to represent each user and each item.\n",
    "\n",
    "- Embeddings are flattened and then concatenated.\n",
    "\n",
    "- After concatenation, the data passes through a dense layer with 64 neurons and ReLU activation, which allows the model to learn non-linear relationships.\n",
    "\n",
    "- The final output layer has 1 neuron (since this is a regression problem where you're predicting a rating value).\n",
    "\n",
    "### Compilation and Training:\n",
    "\n",
    "- The model is compiled with the Adam optimizer and mean squared error (MSE) loss.\n",
    "\n",
    "- The model is trained for 5 epochs with a batch size of 64 and uses 10% of the training data for validation.\n",
    "\n",
    "\n",
    "### Model Evaluation:\n",
    "\n",
    "- After training, the model is evaluated on the test set, resulting in a MSE of approximately 2.61.\n",
    "\n",
    "- The MSE is a measure of the average squared difference between the actual ratings and the ratings predicted by the model. A lower MSE indicates better performance.\n",
    "\n",
    "- This MSE value means that on average, the predicted rating deviates from the actual rating by the square root of 2.612, which is around 1.62. \n",
    "\n",
    "\n",
    "### Interpreting the Results:\n",
    "\n",
    "- The scale of ratings is 1 to 10, an average deviation of around 1.6 might be considered moderate. It shows that the model has learned to some extent but still has room for improvement.\n",
    "\n",
    "\n",
    "### Next Steps for Improvement:\n",
    "\n",
    "- Consider training the model for more epochs or tuning other hyperparameters.\n",
    "- Experiment with different architectures, like adding more dense layers or changing the number of neurons.\n",
    "- Explore advanced techniques like regularization (to prevent overfitting) and learning rate scheduling.\n",
    "- Consider content-based features (like genres or synopses), consider a hybrid model that combines collaborative and content-based approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Deep Learning Models\n",
    "Combine features from both content-based and collaborative filtering in a deep learning architecture, allowing the model to learn complex interactions between content and user preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Based Filtering\n",
    "Feature Extraction from Synopsis using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fill NaN values in 'synopsis' with empty string\n",
    "data_with_emotions['synopsis'].fillna('', inplace=True)\n",
    "\n",
    "# Apply TF-IDF\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "content_features = tfidf.fit_transform(data_with_emotions['synopsis'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Collaborative Filtering Data\n",
    "Prepare user-item interaction data and create embeddings for collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Input, Flatten\n",
    "\n",
    "# Prepare user and item interaction data ('uid_review' and 'anime_uid' as user and item identifiers)\n",
    "user_ids = data_with_emotions['uid_review'].astype('category').cat.codes.values\n",
    "item_ids = data_with_emotions['anime_uid'].astype('category').cat.codes.values\n",
    "\n",
    "num_users = data_with_emotions['uid_review'].nunique()\n",
    "num_items = data_with_emotions['anime_uid'].nunique()\n",
    "embedding_size = 20\n",
    "\n",
    "# User embedding\n",
    "user_input = Input(shape=(1,))\n",
    "user_embedding = Embedding(num_users, embedding_size, input_length=1)(user_input)\n",
    "user_vec = Flatten()(user_embedding)\n",
    "\n",
    "# Item embedding\n",
    "item_input = Input(shape=(1,))\n",
    "item_embedding = Embedding(num_items, embedding_size, input_length=1)(item_input)\n",
    "item_vec = Flatten()(item_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Model Architecture\n",
    "Combine content-based and collaborative filtering features in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# Combine user_vec and item_vec with content features\n",
    "# Assume content_features is a dense representation of content\n",
    "concatenated = concatenate([user_vec, item_vec, content_features])\n",
    "\n",
    "# Add dense layers for deep learning\n",
    "dense = Dense(128, activation='relu')(concatenated)\n",
    "output = Dense(1, activation='linear')(dense)\n",
    "\n",
    "\n",
    "model = Model(inputs=[user_input, item_input, content_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Train the model using your dataset. Note that the dataset needs to be split into training and testing sets, and the content features should be in a compatible format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit([user_ids, item_ids, content_features], small_sample['rating'], epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mood-Based Classification\n",
    "Tagging Anime with Moods from Emotion Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming mood classification has already been done and each anime has a 'mood' column\n",
    "mood_based_data = small_sample[['anime_id', 'mood']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Recommendations Models\n",
    "Combining collaborative and content-based filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendation(user_id, content_rec, collaborative_rec, mood=None):\n",
    "    # Hybrid model combining content and collaborative filtering\n",
    "    hybrid_rec = content_rec * 0.5 + collaborative_rec * 0.5\n",
    "\n",
    "    # If mood is specified, filter or re-rank recommendations\n",
    "    if mood:\n",
    "        hybrid_rec = filter_or_rank_by_mood(hybrid_rec, mood)\n",
    "\n",
    "    return hybrid_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendation(user_id, mood=None):\n",
    "    # Get content-based recommendations\n",
    "    content_based_rec = content_based_model(user_id)\n",
    "\n",
    "    # Get collaborative filtering recommendations\n",
    "    collaborative_rec = collaborative_model(user_id)\n",
    "\n",
    "    # Get mood-based recommendations if a mood is specified\n",
    "    if mood:\n",
    "        mood_based_rec = mood_based_model(user_id, mood)\n",
    "    else:\n",
    "        mood_based_rec = {}\n",
    "\n",
    "    # Combine recommendations\n",
    "    # This is a simple average. Consider using more complex methods\n",
    "    combined_rec = average(content_based_rec, collaborative_rec, mood_based_rec)\n",
    "\n",
    "    return combined_rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalization and Playlist Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_playlist(user_id, mood=None):\n",
    "    content_rec = get_content_based_recommendations(user_id)\n",
    "    collaborative_rec = get_collaborative_recommendations(user_id)\n",
    "    \n",
    "    # Hybrid recommendations\n",
    "    recommendations = hybrid_recommendation(user_id, content_rec, collaborative_rec, mood)\n",
    "\n",
    "    # Generate playlist based on recommendations\n",
    "    playlist = create_playlist(recommendations)\n",
    "    return playlist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Assuming you have true_labels and predicted_labels\n",
    "precision = precision_score(true_labels, predicted_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
