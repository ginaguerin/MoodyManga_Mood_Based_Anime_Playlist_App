{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.1.1-cp39-none-macosx_11_0_arm64.whl (59.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 59.6 MB 36.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.16.1-cp39-cp39-macosx_11_0_arm64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 45.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading torchaudio-2.1.1-cp39-cp39-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Using cached fsspec-2023.12.0-py3-none-any.whl (168 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 44.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 49.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from torch) (4.8.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-10.1.0-cp39-cp39-macosx_11_0_arm64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 57.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting numpy\n",
      "  Downloading numpy-1.26.2-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.0 MB 57.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.3-cp39-cp39-macosx_10_9_universal2.whl (17 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 33.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 36.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 625 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "\u001b[K     |████████████████████████████████| 162 kB 58.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: mpmath, MarkupSafe, urllib3, sympy, networkx, jinja2, idna, fsspec, filelock, charset-normalizer, certifi, torch, requests, pillow, numpy, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-2.1.3 certifi-2023.11.17 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2023.12.0 idna-3.6 jinja2-3.1.2 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.2 pillow-10.1.0 requests-2.31.0 sympy-1.12 torch-2.1.1 torchaudio-2.1.1 torchvision-0.16.1 urllib3-2.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp39-cp39-macosx_11_0_arm64.whl (426 kB)\n",
      "\u001b[K     |████████████████████████████████| 426 kB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (3.13.1)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: requests in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (2.31.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Using cached huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (1.26.2)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.10.3-cp39-cp39-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[K     |████████████████████████████████| 291 kB 40.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[K     |████████████████████████████████| 174 kB 37.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 40.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.1.0)\n",
      "Installing collected packages: tqdm, pyyaml, huggingface-hub, tokenizers, safetensors, regex, transformers\n",
      "Successfully installed huggingface-hub-0.19.4 pyyaml-6.0.1 regex-2023.10.3 safetensors-0.4.1 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.3.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.35.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages\n",
      "Requires: tqdm, regex, tokenizers, pyyaml, numpy, safetensors, packaging, filelock, huggingface-hub, requests\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting s3fs\n",
      "  Using cached s3fs-2023.12.0-py3-none-any.whl (29 kB)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4\n",
      "  Using cached aiobotocore-2.8.0-py3-none-any.whl (75 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.9.1-cp39-cp39-macosx_11_0_arm64.whl (387 kB)\n",
      "\u001b[K     |████████████████████████████████| 387 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec==2023.12.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from s3fs) (2023.12.0)\n",
      "Collecting wrapt<2.0.0,>=1.10.10\n",
      "  Downloading wrapt-1.16.0-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting botocore<1.33.2,>=1.32.4\n",
      "  Using cached botocore-1.33.1-py3-none-any.whl (11.6 MB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp39-cp39-macosx_11_0_arm64.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.3-cp39-cp39-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 26.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attrs>=17.3.0\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 17.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from aioitertools<1.0.0,>=0.5.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from botocore<1.33.2,>=1.32.4->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.8.2)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[K     |████████████████████████████████| 143 kB 38.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.33.2,>=1.32.4->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.15.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.6)\n",
      "Installing collected packages: multidict, frozenlist, yarl, urllib3, jmespath, attrs, async-timeout, aiosignal, wrapt, botocore, aioitertools, aiohttp, aiobotocore, s3fs\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "Successfully installed aiobotocore-2.8.0 aiohttp-3.9.1 aioitertools-0.11.0 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 botocore-1.33.1 frozenlist-1.4.0 jmespath-1.0.1 multidict-6.0.4 s3fs-2023.12.0 urllib3-1.26.18 wrapt-1.16.0 yarl-1.9.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fsspec==2023.3.0\n",
      "  Using cached fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "Installing collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.12.0\n",
      "    Uninstalling fsspec-2023.12.0:\n",
      "      Successfully uninstalled fsspec-2023.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.12.0 requires fsspec==2023.12.0, but you have fsspec 2023.3.0 which is incompatible.\n",
      "huggingface-hub 0.19.4 requires fsspec>=2023.5.0, but you have fsspec 2023.3.0 which is incompatible.\u001b[0m\n",
      "Successfully installed fsspec-2023.3.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install fsspec==2023.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from nltk) (2023.10.3)\n",
      "Collecting click\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: tqdm in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from nltk) (4.66.1)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 44.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.1.3-cp39-cp39-macosx_11_0_arm64.whl (11.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.0 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[K     |████████████████████████████████| 341 kB 40.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[K     |████████████████████████████████| 502 kB 52.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.22.4 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.1.3 pytz-2023.3.post1 tzdata-2023.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (1.26.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.26.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kariprimiano/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 89] Operation canceled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/kariprimiano/Documents/Phase4_EDA++/DRAFT_EDA_nb.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kariprimiano/Documents/Phase4_EDA%2B%2B/DRAFT_EDA_nb.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m master_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mmaster_anime_data.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1724\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m kwds[\u001b[39m\"\u001b[39m\u001b[39mdtype_backend\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[39m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39;49mTextReader(src, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 89] Operation canceled"
     ]
    }
   ],
   "source": [
    "#master_data = pd.read_csv('master_anime_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset of your data for processing\n",
    "#sampled_data = master_data.sample(frac=0.1, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = pd.read_csv('sample_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kariprimiano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synopsis</th>\n",
       "      <th>emotion_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After the accident in which she lost her mothe...</td>\n",
       "      <td>{'anger': 0.010413373820483685, 'disgust': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naota Nandaba is an ordinary sixth grader livi...</td>\n",
       "      <td>{'anger': 0.18671022355556488, 'disgust': 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Story begins as Momiji's chastity is taken by ...</td>\n",
       "      <td>{'anger': 0.007550527341663837, 'disgust': 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tanya Degurechaff is a young soldier infamous ...</td>\n",
       "      <td>{'anger': 0.5558895468711853, 'disgust': 0.003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hachiman Hikigaya is an apathetic high school ...</td>\n",
       "      <td>{'anger': 0.011294235475361347, 'disgust': 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            synopsis  \\\n",
       "0  After the accident in which she lost her mothe...   \n",
       "1  Naota Nandaba is an ordinary sixth grader livi...   \n",
       "2  Story begins as Momiji's chastity is taken by ...   \n",
       "3  Tanya Degurechaff is a young soldier infamous ...   \n",
       "4  Hachiman Hikigaya is an apathetic high school ...   \n",
       "\n",
       "                                      emotion_scores  \n",
       "0  {'anger': 0.010413373820483685, 'disgust': 0.0...  \n",
       "1  {'anger': 0.18671022355556488, 'disgust': 0.00...  \n",
       "2  {'anger': 0.007550527341663837, 'disgust': 0.0...  \n",
       "3  {'anger': 0.5558895468711853, 'disgust': 0.003...  \n",
       "4  {'anger': 0.011294235475361347, 'disgust': 0.0...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download nltk stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the emotion classification pipeline\n",
    "model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "emotion_classifier = pipeline('text-classification', model=model_name, return_all_scores=True)\n",
    "\n",
    "# Set English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for advanced text cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Function to classify emotions of a text\n",
    "def classify_emotions(text):\n",
    "    cleaned_text = clean_text(text)  # Clean the text using clean_text function\n",
    "    predictions = emotion_classifier(cleaned_text)\n",
    "    return {emotion['label']: emotion['score'] for emotion in predictions[0]}\n",
    "\n",
    "# Apply the model to the 'synopsis' column\n",
    "sampled_data['emotion_scores'] = sampled_data['synopsis'].astype(str).apply(classify_emotions)\n",
    "\n",
    "# Inspect results\n",
    "sampled_data[['synopsis', 'emotion_scores']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0        {'anger': 0.010413373820483685, 'disgust': 0.0...\n",
       "1        {'anger': 0.18671022355556488, 'disgust': 0.00...\n",
       "2        {'anger': 0.007550527341663837, 'disgust': 0.0...\n",
       "3        {'anger': 0.5558895468711853, 'disgust': 0.003...\n",
       "4        {'anger': 0.011294235475361347, 'disgust': 0.0...\n",
       "                               ...                        \n",
       "34983    {'anger': 0.0200864989310503, 'disgust': 0.000...\n",
       "34984    {'anger': 0.007323819678276777, 'disgust': 0.0...\n",
       "34985    {'anger': 0.005832878407090902, 'disgust': 0.0...\n",
       "34986    {'anger': 0.004111292771995068, 'disgust': 0.0...\n",
       "34987    {'anger': 0.022216347977519035, 'disgust': 0.0...\n",
       "Name: emotion_scores, Length: 34988, dtype: object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data['emotion_scores'].value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding 'emotion_scores' into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid_review</th>\n",
       "      <th>profile</th>\n",
       "      <th>anime_uid</th>\n",
       "      <th>scores</th>\n",
       "      <th>Overall</th>\n",
       "      <th>Story</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Sound</th>\n",
       "      <th>Character</th>\n",
       "      <th>Enjoyment</th>\n",
       "      <th>...</th>\n",
       "      <th>favorites_anime</th>\n",
       "      <th>link_y</th>\n",
       "      <th>emotion_scores</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>neutral</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27270</td>\n",
       "      <td>Elfsire</td>\n",
       "      <td>120</td>\n",
       "      <td>{'Overall': '8', 'Story': '8', 'Animation': '1...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>['14713', '14289', '4081', '14467', '16498', '...</td>\n",
       "      <td>https://myanimelist.net/profile/Elfsire</td>\n",
       "      <td>{'anger': 0.010413373820483685, 'disgust': 0.0...</td>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.197999</td>\n",
       "      <td>0.028194</td>\n",
       "      <td>0.013339</td>\n",
       "      <td>0.689401</td>\n",
       "      <td>0.059908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228044</td>\n",
       "      <td>merryfistmas</td>\n",
       "      <td>227</td>\n",
       "      <td>{'Overall': '10', 'Story': '10', 'Animation': ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>['18679', '22135', '227', '23847', '5114', '11...</td>\n",
       "      <td>https://myanimelist.net/profile/merryfistmas</td>\n",
       "      <td>{'anger': 0.18671022355556488, 'disgust': 0.00...</td>\n",
       "      <td>0.186710</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.069133</td>\n",
       "      <td>0.091217</td>\n",
       "      <td>0.139144</td>\n",
       "      <td>0.045635</td>\n",
       "      <td>0.465496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21744</td>\n",
       "      <td>ravenrage</td>\n",
       "      <td>3303</td>\n",
       "      <td>{'Overall': '6', 'Story': '3', 'Animation': '8...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>['134']</td>\n",
       "      <td>https://myanimelist.net/profile/ravenrage</td>\n",
       "      <td>{'anger': 0.007550527341663837, 'disgust': 0.0...</td>\n",
       "      <td>0.007551</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.658758</td>\n",
       "      <td>0.077451</td>\n",
       "      <td>0.017352</td>\n",
       "      <td>0.235827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>248039</td>\n",
       "      <td>Tyrannicswine117</td>\n",
       "      <td>32615</td>\n",
       "      <td>{'Overall': '8', 'Story': '9', 'Animation': '8...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>['5114', '4181', '9253', '18679', '1', '2001',...</td>\n",
       "      <td>https://myanimelist.net/profile/Tyrannicswine117</td>\n",
       "      <td>{'anger': 0.5558895468711853, 'disgust': 0.003...</td>\n",
       "      <td>0.555890</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.173663</td>\n",
       "      <td>0.078683</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>0.014780</td>\n",
       "      <td>0.082877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>215510</td>\n",
       "      <td>ThePunzalan</td>\n",
       "      <td>14813</td>\n",
       "      <td>{'Overall': '9', 'Story': '9', 'Animation': '9...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>['14813', '30831', '15809', '4224', '31715', '...</td>\n",
       "      <td>https://myanimelist.net/profile/ThePunzalan</td>\n",
       "      <td>{'anger': 0.011294235475361347, 'disgust': 0.0...</td>\n",
       "      <td>0.011294</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.938779</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>0.031252</td>\n",
       "      <td>0.009808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid_review           profile  anime_uid  \\\n",
       "0       27270           Elfsire        120   \n",
       "1      228044      merryfistmas        227   \n",
       "2       21744         ravenrage       3303   \n",
       "3      248039  Tyrannicswine117      32615   \n",
       "4      215510       ThePunzalan      14813   \n",
       "\n",
       "                                              scores  Overall  Story  \\\n",
       "0  {'Overall': '8', 'Story': '8', 'Animation': '1...        8      8   \n",
       "1  {'Overall': '10', 'Story': '10', 'Animation': ...       10     10   \n",
       "2  {'Overall': '6', 'Story': '3', 'Animation': '8...        6      3   \n",
       "3  {'Overall': '8', 'Story': '9', 'Animation': '8...        8      9   \n",
       "4  {'Overall': '9', 'Story': '9', 'Animation': '9...        9      9   \n",
       "\n",
       "   Animation  Sound  Character  Enjoyment  ...  \\\n",
       "0         10      8         10          7  ...   \n",
       "1         10     10         10         10  ...   \n",
       "2          8      5          6          7  ...   \n",
       "3          8      8          9          8  ...   \n",
       "4          9      9         10          9  ...   \n",
       "\n",
       "                                     favorites_anime  \\\n",
       "0  ['14713', '14289', '4081', '14467', '16498', '...   \n",
       "1  ['18679', '22135', '227', '23847', '5114', '11...   \n",
       "2                                            ['134']   \n",
       "3  ['5114', '4181', '9253', '18679', '1', '2001',...   \n",
       "4  ['14813', '30831', '15809', '4224', '31715', '...   \n",
       "\n",
       "                                             link_y  \\\n",
       "0           https://myanimelist.net/profile/Elfsire   \n",
       "1      https://myanimelist.net/profile/merryfistmas   \n",
       "2         https://myanimelist.net/profile/ravenrage   \n",
       "3  https://myanimelist.net/profile/Tyrannicswine117   \n",
       "4       https://myanimelist.net/profile/ThePunzalan   \n",
       "\n",
       "                                      emotion_scores     anger   disgust  \\\n",
       "0  {'anger': 0.010413373820483685, 'disgust': 0.0...  0.010413  0.000746   \n",
       "1  {'anger': 0.18671022355556488, 'disgust': 0.00...  0.186710  0.002664   \n",
       "2  {'anger': 0.007550527341663837, 'disgust': 0.0...  0.007551  0.001111   \n",
       "3  {'anger': 0.5558895468711853, 'disgust': 0.003...  0.555890  0.003051   \n",
       "4  {'anger': 0.011294235475361347, 'disgust': 0.0...  0.011294  0.000683   \n",
       "\n",
       "       fear       joy   neutral   sadness  surprise  \n",
       "0  0.197999  0.028194  0.013339  0.689401  0.059908  \n",
       "1  0.069133  0.091217  0.139144  0.045635  0.465496  \n",
       "2  0.001951  0.658758  0.077451  0.017352  0.235827  \n",
       "3  0.173663  0.078683  0.091056  0.014780  0.082877  \n",
       "4  0.000702  0.938779  0.007481  0.031252  0.009808  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the emotion_scores column needs to be parsed from string to dictionary\n",
    "if isinstance(sampled_data['emotion_scores'].iloc[0], str):\n",
    "    sampled_data['emotion_scores'] = sampled_data['emotion_scores'].apply(ast.literal_eval)\n",
    "\n",
    "# Expand the emotion_scores column into separate columns\n",
    "emotion_scores = sampled_data['emotion_scores'].apply(pd.Series)\n",
    "\n",
    "# Join the expanded emotion scores dataframe with the original data\n",
    "data_with_emotions = sampled_data.join(emotion_scores)\n",
    "\n",
    "# Display the first few rows to verify the expansion\n",
    "data_with_emotions.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model for Mood Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-Based Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mood_label\n",
       "Fear          8975\n",
       "Neutral       7069\n",
       "Sad           5518\n",
       "Happy         4686\n",
       "Angry         4410\n",
       "Excitement    4315\n",
       "Disgust         15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_mood_label(row):\n",
    "    if row['joy'] > 0.5:\n",
    "        return 'Happy'\n",
    "    elif row['sadness'] > 0.5:\n",
    "        return 'Sad'\n",
    "    elif row['anger'] > 0.5:\n",
    "        return 'Angry'\n",
    "    elif row['disgust'] > 0.5:\n",
    "        return 'Disgust'\n",
    "    elif row['fear'] > 0.5:\n",
    "        return 'Fear'\n",
    "    elif row['surprise'] > 0.5:\n",
    "        return 'Excitement'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "data_with_emotions['mood_label'] = data_with_emotions.apply(create_mood_label, axis=1)\n",
    "data_with_emotions['mood_label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "# Prepare the data - Features and Labels\n",
    "X = data_with_emotions[['uid_anime', 'title', 'genre', 'popularity']]\n",
    "y = data_with_emotions['mood_label']\n",
    "\n",
    "numerical_feature = ['uid_anime', 'popularity']\n",
    "categorical_feature = ['title', 'genre']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "\n",
    "# Categorical feature preprocessing\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Numerical feature preprocessing with missing value handling\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing for different types of features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_feature),\n",
    "        ('num', numerical_transformer, numerical_feature)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "# We need to apply preprocessing to train and test sets separately\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: 0.98\n",
      "Accuracy on Test Data: 0.98\n",
      "Confusion Matrix:\n",
      " [[ 840    0    0    0    1   16    0]\n",
      " [   0    3    0    0    0    0    0]\n",
      " [   0    0  831    0    0   23    0]\n",
      " [   0    0    1 1756    1   21    0]\n",
      " [   0    0    0    0  936   29    0]\n",
      " [   1    0    0    0    2 1414    0]\n",
      " [   0    0    0    0    0   25 1098]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the classifier\n",
    "classifier = RandomForestClassifier(random_state=13)\n",
    "\n",
    "# Train the classifier with transformed training data\n",
    "classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Perform cross-validation\n",
    "# Note: For cross-validation, you need to preprocess the entire dataset\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "cross_val_scores = cross_val_score(classifier, X_transformed, y, cv=5)\n",
    "print(f'Cross-Validation Scores: {cross_val_scores.mean():.2f}')\n",
    "\n",
    "# Make predictions on the transformed test set\n",
    "y_pred = classifier.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy on Test Data: {accuracy:.2f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:\\n', cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Features\n",
    "- Numerical Features (uid_anime, popularity): These features likely provide information about the identity and popularity of each anime. uid_anime is a unique identifier, and popularity reflects how well-known or frequently watched an anime is.\n",
    "\n",
    "- Categorical Features (title, genre): The title may not offer much predictive power due to its uniqueness for each anime, but genre is crucial as it categorically describes the type of content in an anime, which can significantly influence its mood.\n",
    "\n",
    "### Cross-Validation Scores\n",
    "Mean of Cross-Validation Scores: Averaging at {cross_val_scores.mean():.2f}, this metric indicates the model's average performance across different subsets of the dataset. A higher score suggests better generalization, meaning the model performs consistently across different data samples.\n",
    "\n",
    "### Accuracy on Test Data\n",
    "- Accuracy: With an accuracy of {accuracy:.2f} on the test set, this metric tells you how often the model correctly predicts the mood label. A high accuracy is desirable, indicating that the model performs well on unseen data. However, accuracy alone might not give a complete picture, especially if the dataset is imbalanced (i.e., some mood labels are much more common than others).\n",
    "\n",
    "### Confusion Matrix\n",
    "Confusion Matrix: The confusion matrix cm provides a detailed breakdown of the model's predictions. It shows the number of correct and incorrect predictions for each mood label, classified into true positives, false positives, true negatives, and false negatives. Analyzing the confusion matrix can help identify if the model is particularly good or bad at predicting specific mood labels.\n",
    "\n",
    "### Interpretation and Considerations\n",
    "- The model shows high accuracy and consistent cross-validation scores, indicating that the chosen features (especially genre and popularity) are good predictors for the mood label of an anime.\n",
    "\n",
    "- The usefulness of uid_anime and title in predicting mood might be limited. uid_anime is more about identification, and title varies widely.\n",
    "\n",
    "- If there are mood labels with poor prediction rates, as suggested by the confusion matrix, it could indicate a need for more representative data for those labels or a reevaluation of the features used.\n",
    "\n",
    "- Consider the balance of your dataset. If some mood labels are underrepresented, the model's ability to predict those moods might be limited, which can be reflected in the confusion matrix.\n",
    "\n",
    "### Conclusion\n",
    "Overall, the combination of genre and popularity as features seems to be key in predicting the mood of an anime, supplemented by the unique identifiers of each anime. The model's performance as indicated by the accuracy and confusion matrix suggests how well these features work together to predict moods in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Classification Model Using Popularity & Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data - Features and Labels\n",
    "X = data_with_emotions[['genre', 'popularity']]\n",
    "y = data_with_emotions['mood_label']\n",
    "\n",
    "numerical_feature = ['popularity']\n",
    "categorical_feature = ['genre']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "\n",
    "# Categorical feature preprocessing\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Numerical feature preprocessing with missing value handling\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing for different types of features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_feature),\n",
    "        ('num', numerical_transformer, numerical_feature)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "# We need to apply preprocessing to train and test sets separately\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: 0.98\n",
      "Accuracy on Test Data: 0.98\n",
      "Confusion Matrix:\n",
      " [[ 840    0    1    0    2   12    2]\n",
      " [   0    3    0    0    0    0    0]\n",
      " [   1    0  832    0    1   20    0]\n",
      " [   1    0    2 1758    2   15    1]\n",
      " [   1    0    2    0  939   21    2]\n",
      " [   5    0    3    4    6 1398    1]\n",
      " [   0    0    0    1    1   22 1099]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the classifier\n",
    "mood_classifier = RandomForestClassifier(random_state=13)\n",
    "\n",
    "# Train the classifier with transformed training data\n",
    "mood_classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Perform cross-validation\n",
    "# Note: For cross-validation, you need to preprocess the entire dataset\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "cross_val_scores = cross_val_score(mood_classifier, X_transformed, y, cv=5)\n",
    "print(f'Cross-Validation Scores: {cross_val_scores.mean():.2f}')\n",
    "\n",
    "# Make predictions on the transformed test set\n",
    "y_pred = mood_classifier.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy on Test Data: {accuracy:.2f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:\\n', cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Super-users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for super-users (top 5%): 56.00\n",
      "Identified 970 super-users.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of interactions per user\n",
    "interaction_counts = master_data.groupby('profile').size()\n",
    "\n",
    "# Using percentiles to define super-users\n",
    "percentile_threshold = 95  # Top 5% of users\n",
    "threshold = interaction_counts.quantile(percentile_threshold / 100)\n",
    "super_users = interaction_counts[interaction_counts > threshold].index.tolist()\n",
    "\n",
    "print(f\"Threshold for super-users (top {100 - percentile_threshold}%): {threshold:.2f}\")\n",
    "print(f\"Identified {len(super_users)} super-users.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data_with_emotions subset - approx. 34k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold for super-users (top 5%): 10.00\n",
      "Identified 497 super-users.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of interactions per user\n",
    "interaction_counts = data_with_emotions.groupby('profile').size()\n",
    "\n",
    "# Using percentiles to define super-users\n",
    "percentile_threshold = 95  # Top 5% of users\n",
    "threshold = interaction_counts.quantile(percentile_threshold / 100)\n",
    "super_users = interaction_counts[interaction_counts > threshold].index.tolist()\n",
    "\n",
    "print(f\"Threshold for super-users (top {100 - percentile_threshold}%): {threshold:.2f}\")\n",
    "print(f\"Identified {len(super_users)} super-users.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New dataset with and without super-users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#super_users = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Collaborative Filtering (NCF)\n",
    "NCF models use neural networks to learn user-item interaction patterns, providing a more flexible and powerful way to model complex relationships compared to traditional matrix factorization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp39-cp39-macosx_12_0_arm64.whl (2.1 kB)\n",
      "Collecting tensorflow-macos==2.15.0\n",
      "  Downloading tensorflow_macos-2.15.0-cp39-cp39-macosx_12_0_arm64.whl (208.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 208.8 MB 8.1 kB/s  eta 0:00:01████████████████▍             | 119.6 MB 79.2 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[K     |████████████████████████████████| 394 kB 72.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers>=23.5.26\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-macosx_10_9_universal2.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 44.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.15.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.26.2)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp39-cp39-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 39.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.10.0-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 80.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-macosx_11_0_arm64.whl (35 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow-macos==2.15.0->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: packaging in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.2)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-macosx_11_0_arm64.whl (20.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.6 MB 46.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.59.3-cp39-cp39-macosx_10_10_universal2.whl (9.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.6 MB 66.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<2.16,>=2.15\n",
      "  Using cached tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.8.0)\n",
      "Collecting keras<2.16,>=2.15.0\n",
      "  Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.37.0)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "\u001b[K     |████████████████████████████████| 102 kB 33.2 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[K     |████████████████████████████████| 226 kB 54.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.24.0-py2.py3-none-any.whl (183 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 75.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.17.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.6)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.3)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, ml-dtypes, libclang, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow-macos, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.24.0 google-auth-oauthlib-1.1.0 google-pasta-0.2.0 grpcio-1.59.3 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 markdown-3.5.1 ml-dtypes-0.2.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.34.0 tensorflow-macos-2.15.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.14.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (2.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Input, concatenate, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "\n",
    "# Columns: 'uid_review' for users, 'anime_uid' for items, and 'Overall' for ratings\n",
    "# Convert user and item IDs to categorical variables\n",
    "data_with_emotions['uid_review'] = data_with_emotions['uid_review'].astype('category')\n",
    "data_with_emotions['anime_uid'] = data_with_emotions['anime_uid'].astype('category')\n",
    "\n",
    "# Map user and item IDs to integers\n",
    "user_id_mapping = {id: i for i, id in enumerate(data_with_emotions['uid_review'].cat.categories)}\n",
    "item_id_mapping = {id: i for i, id in enumerate(data_with_emotions['anime_uid'].cat.categories)}\n",
    "\n",
    "# Apply mapping\n",
    "data_with_emotions['user'] = data_with_emotions['uid_review'].map(user_id_mapping)\n",
    "data_with_emotions['item'] = data_with_emotions['anime_uid'].map(item_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "394/394 [==============================] - 1s 2ms/step - loss: 20.6236 - val_loss: 3.8344\n",
      "Epoch 2/5\n",
      "394/394 [==============================] - 1s 2ms/step - loss: 2.0858 - val_loss: 2.7484\n",
      "Epoch 3/5\n",
      "394/394 [==============================] - 1s 2ms/step - loss: 0.5856 - val_loss: 2.6623\n",
      "Epoch 4/5\n",
      "394/394 [==============================] - 1s 2ms/step - loss: 0.3072 - val_loss: 2.6076\n",
      "Epoch 5/5\n",
      "394/394 [==============================] - 1s 2ms/step - loss: 0.2155 - val_loss: 2.4720\n",
      "219/219 [==============================] - 0s 297us/step - loss: 2.5418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.5418050289154053"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the data for the model\n",
    "X = data_with_emotions[['user', 'item']].values\n",
    "y = data_with_emotions['Overall'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "# Neural Collaborative Filtering model architecture\n",
    "user_input = Input(shape=(1,))\n",
    "item_input = Input(shape=(1,))\n",
    "user_embedding = Embedding(len(user_id_mapping), 15, input_length=1)(user_input)\n",
    "item_embedding = Embedding(len(item_id_mapping), 15, input_length=1)(item_input)\n",
    "user_vec = Flatten()(user_embedding)\n",
    "item_vec = Flatten()(item_embedding)\n",
    "concat = concatenate([user_vec, item_vec])\n",
    "dense = Dense(64, activation='relu')(concat)\n",
    "output = Dense(1)(dense)\n",
    "model = Model([user_input, item_input], output)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "model.fit([X_train[:, 0], X_train[:, 1]], y_train, batch_size=64, epochs=5, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate([X_test[:, 0], X_test[:, 1]], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a Neural Collaborative Filtering (NCF) model, which is designed to predict user ratings for items (anime) based on learned user and item embeddings.\n",
    "\n",
    "### Model Architecture:\n",
    "\n",
    "- Model has separate embedding layers for users (user_embedding) and items (item_embedding), each with an embedding size of 15. This means the model learns a 15-dimensional vector to represent each user and each item.\n",
    "\n",
    "- Embeddings are flattened and then concatenated.\n",
    "\n",
    "- After concatenation, the data passes through a dense layer with 64 neurons and ReLU activation, which allows the model to learn non-linear relationships.\n",
    "\n",
    "- The final output layer has 1 neuron (since this is a regression problem where you're predicting a rating value).\n",
    "\n",
    "### Compilation and Training:\n",
    "\n",
    "- The model is compiled with the Adam optimizer and mean squared error (MSE) loss.\n",
    "\n",
    "- The model is trained for 5 epochs with a batch size of 64 and uses 10% of the training data for validation.\n",
    "\n",
    "\n",
    "### Model Evaluation:\n",
    "\n",
    "- After training, the model is evaluated on the test set, resulting in a MSE of approximately 2.61.\n",
    "\n",
    "- The MSE is a measure of the average squared difference between the actual ratings and the ratings predicted by the model. A lower MSE indicates better performance.\n",
    "\n",
    "- This MSE value means that on average, the predicted rating deviates from the actual rating by the square root of 2.612, which is around 1.62. \n",
    "\n",
    "\n",
    "### Interpreting the Results:\n",
    "\n",
    "- The scale of ratings is 1 to 10, an average deviation of around 1.6 might be considered moderate. It shows that the model has learned to some extent but still has room for improvement.\n",
    "\n",
    "\n",
    "### Next Steps for Improvement:\n",
    "\n",
    "- Consider training the model for more epochs or tuning other hyperparameters.\n",
    "- Experiment with different architectures, like adding more dense layers or changing the number of neurons.\n",
    "- Explore advanced techniques like regularization (to prevent overfitting) and learning rate scheduling.\n",
    "- Consider content-based features (like genres or synopses), consider a hybrid model that combines collaborative and content-based approaches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Deep Learning Models\n",
    "Combine features from both content-based and collaborative filtering in a deep learning architecture, allowing the model to learn complex interactions between content and user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-surprise\n",
      "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "\u001b[K     |████████████████████████████████| 771 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from scikit-surprise) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from scikit-surprise) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from scikit-surprise) (1.11.4)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/zg/2vpc_k5518g10mcclm90c_sw0000gn/T/pip-install-mxlyhv_9/scikit-surprise_2e6b0445d5dc46869fe3afbc2577d17f/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/zg/2vpc_k5518g10mcclm90c_sw0000gn/T/pip-install-mxlyhv_9/scikit-surprise_2e6b0445d5dc46869fe3afbc2577d17f/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /private/var/folders/zg/2vpc_k5518g10mcclm90c_sw0000gn/T/pip-wheel-4h7z3_dx\n",
      "       cwd: /private/var/folders/zg/2vpc_k5518g10mcclm90c_sw0000gn/T/pip-install-mxlyhv_9/scikit-surprise_2e6b0445d5dc46869fe3afbc2577d17f/\n",
      "  Complete output (54 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-10.9-universal2-3.9\n",
      "  creating build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "  copying surprise/builtin_datasets.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "  copying surprise/__init__.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "  copying surprise/dump.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "  copying surprise/dataset.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "  copying surprise/reader.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "  copying surprise/utils.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "  copying surprise/trainset.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "  copying surprise/accuracy.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "  copying surprise/__main__.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "  creating build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/algo_base.py -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/knns.py -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/predictions.py -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/__init__.py -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/random_pred.py -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/baseline_only.py -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  creating build/lib.macosx-10.9-universal2-3.9/surprise/model_selection\n",
      "  copying surprise/model_selection/__init__.py -> build/lib.macosx-10.9-universal2-3.9/surprise/model_selection\n",
      "  copying surprise/model_selection/split.py -> build/lib.macosx-10.9-universal2-3.9/surprise/model_selection\n",
      "  copying surprise/model_selection/search.py -> build/lib.macosx-10.9-universal2-3.9/surprise/model_selection\n",
      "  copying surprise/model_selection/validation.py -> build/lib.macosx-10.9-universal2-3.9/surprise/model_selection\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info/PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info/dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info/entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info/requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info/top_level.txt\n",
      "  reading manifest file 'scikit_surprise.egg-info/SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info/SOURCES.txt'\n",
      "  copying surprise/similarities.c -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "  copying surprise/similarities.pyx -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "  copying surprise/prediction_algorithms/co_clustering.c -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/co_clustering.pyx -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/matrix_factorization.c -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/matrix_factorization.pyx -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/optimize_baselines.c -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/optimize_baselines.pyx -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/slope_one.c -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  copying surprise/prediction_algorithms/slope_one.pyx -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  creating build/temp.macosx-10.9-universal2-3.9\n",
      "  creating build/temp.macosx-10.9-universal2-3.9/surprise\n",
      "  x86_64-apple-darwin13.4.0-clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/Headers -arch arm64 -arch x86_64 -Werror=implicit-function-declaration -Wno-error=unreachable-code -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /Users/kariprimiano/anaconda3/envs/learn-env/include -D_FORTIFY_SOURCE=2 -mmacosx-version-min=10.9 -isystem /Users/kariprimiano/anaconda3/envs/learn-env/include -I/Users/kariprimiano/Library/Python/3.9/lib/python/site-packages/numpy/core/include -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/include/python3.9 -c surprise/similarities.c -o build/temp.macosx-10.9-universal2-3.9/surprise/similarities.o\n",
      "  clang-10: error: the clang compiler does not support '-march=core2'\n",
      "  error: command '/Users/kariprimiano/anaconda3/envs/learn-env/bin/x86_64-apple-darwin13.4.0-clang' failed with exit code 1\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for scikit-surprise\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for scikit-surprise\n",
      "Failed to build scikit-surprise\n",
      "Installing collected packages: scikit-surprise\n",
      "    Running setup.py install for scikit-surprise ... \u001b[?25lerror\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/zg/2vpc_k5518g10mcclm90c_sw0000gn/T/pip-install-mxlyhv_9/scikit-surprise_2e6b0445d5dc46869fe3afbc2577d17f/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/zg/2vpc_k5518g10mcclm90c_sw0000gn/T/pip-install-mxlyhv_9/scikit-surprise_2e6b0445d5dc46869fe3afbc2577d17f/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/zg/2vpc_k5518g10mcclm90c_sw0000gn/T/pip-record-wxmkh67i/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /Users/kariprimiano/Library/Python/3.9/include/python3.9/scikit-surprise\n",
      "         cwd: /private/var/folders/zg/2vpc_k5518g10mcclm90c_sw0000gn/T/pip-install-mxlyhv_9/scikit-surprise_2e6b0445d5dc46869fe3afbc2577d17f/\n",
      "    Complete output (54 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.macosx-10.9-universal2-3.9\n",
      "    creating build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "    copying surprise/builtin_datasets.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "    copying surprise/__init__.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "    copying surprise/dump.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "    copying surprise/dataset.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "    copying surprise/reader.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "    copying surprise/utils.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "    copying surprise/trainset.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "    copying surprise/accuracy.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "    copying surprise/__main__.py -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "    creating build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/algo_base.py -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/knns.py -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/predictions.py -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/__init__.py -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/random_pred.py -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/baseline_only.py -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    creating build/lib.macosx-10.9-universal2-3.9/surprise/model_selection\n",
      "    copying surprise/model_selection/__init__.py -> build/lib.macosx-10.9-universal2-3.9/surprise/model_selection\n",
      "    copying surprise/model_selection/split.py -> build/lib.macosx-10.9-universal2-3.9/surprise/model_selection\n",
      "    copying surprise/model_selection/search.py -> build/lib.macosx-10.9-universal2-3.9/surprise/model_selection\n",
      "    copying surprise/model_selection/validation.py -> build/lib.macosx-10.9-universal2-3.9/surprise/model_selection\n",
      "    running egg_info\n",
      "    writing scikit_surprise.egg-info/PKG-INFO\n",
      "    writing dependency_links to scikit_surprise.egg-info/dependency_links.txt\n",
      "    writing entry points to scikit_surprise.egg-info/entry_points.txt\n",
      "    writing requirements to scikit_surprise.egg-info/requires.txt\n",
      "    writing top-level names to scikit_surprise.egg-info/top_level.txt\n",
      "    reading manifest file 'scikit_surprise.egg-info/SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    adding license file 'LICENSE.md'\n",
      "    writing manifest file 'scikit_surprise.egg-info/SOURCES.txt'\n",
      "    copying surprise/similarities.c -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "    copying surprise/similarities.pyx -> build/lib.macosx-10.9-universal2-3.9/surprise\n",
      "    copying surprise/prediction_algorithms/co_clustering.c -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/co_clustering.pyx -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/matrix_factorization.c -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/matrix_factorization.pyx -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/optimize_baselines.c -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/optimize_baselines.pyx -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/slope_one.c -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    copying surprise/prediction_algorithms/slope_one.pyx -> build/lib.macosx-10.9-universal2-3.9/surprise/prediction_algorithms\n",
      "    running build_ext\n",
      "    building 'surprise.similarities' extension\n",
      "    creating build/temp.macosx-10.9-universal2-3.9\n",
      "    creating build/temp.macosx-10.9-universal2-3.9/surprise\n",
      "    x86_64-apple-darwin13.4.0-clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -iwithsysroot/System/Library/Frameworks/System.framework/PrivateHeaders -iwithsysroot/Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/Headers -arch arm64 -arch x86_64 -Werror=implicit-function-declaration -Wno-error=unreachable-code -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /Users/kariprimiano/anaconda3/envs/learn-env/include -D_FORTIFY_SOURCE=2 -mmacosx-version-min=10.9 -isystem /Users/kariprimiano/anaconda3/envs/learn-env/include -I/Users/kariprimiano/Library/Python/3.9/lib/python/site-packages/numpy/core/include -I/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/include/python3.9 -c surprise/similarities.c -o build/temp.macosx-10.9-universal2-3.9/surprise/similarities.o\n",
      "    clang-10: error: the clang compiler does not support '-march=core2'\n",
      "    error: command '/Users/kariprimiano/anaconda3/envs/learn-env/bin/x86_64-apple-darwin13.4.0-clang' failed with exit code 1\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Command errored out with exit status 1: /Library/Developer/CommandLineTools/usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/private/var/folders/zg/2vpc_k5518g10mcclm90c_sw0000gn/T/pip-install-mxlyhv_9/scikit-surprise_2e6b0445d5dc46869fe3afbc2577d17f/setup.py'\"'\"'; __file__='\"'\"'/private/var/folders/zg/2vpc_k5518g10mcclm90c_sw0000gn/T/pip-install-mxlyhv_9/scikit-surprise_2e6b0445d5dc46869fe3afbc2577d17f/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /private/var/folders/zg/2vpc_k5518g10mcclm90c_sw0000gn/T/pip-record-wxmkh67i/install-record.txt --single-version-externally-managed --user --prefix= --compile --install-headers /Users/kariprimiano/Library/Python/3.9/include/python3.9/scikit-surprise Check the logs for full command output.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Based Filtering\n",
    "Feature Extraction from Synopsis using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding, Input, Flatten, concatenate, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Fill NaN values in 'synopsis' with empty string\n",
    "data_with_emotions['synopsis'].fillna('', inplace=True)\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = train_test_split(data_with_emotions, test_size=0.2, random_state=13)\n",
    "\n",
    "# Apply TF-IDF to the training set and transform the test set\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "train_content_features = tfidf.fit_transform(train_data['synopsis'])\n",
    "test_content_features = tfidf.transform(test_data['synopsis'])\n",
    "\n",
    "# Convert sparse TF-IDF matrix to a dense format for both train and test sets\n",
    "train_content_features_dense = train_content_features.todense()\n",
    "test_content_features_dense = test_content_features.todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Collaborative Filtering Data\n",
    "Prepare user-item interaction data and create embeddings for collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare user and item interaction data for both train and test sets\n",
    "train_user_ids = train_data['uid_review'].astype('category').cat.codes.values\n",
    "train_item_ids = train_data['anime_uid'].astype('category').cat.codes.values\n",
    "test_user_ids = test_data['uid_review'].astype('category').cat.codes.values\n",
    "test_item_ids = test_data['anime_uid'].astype('category').cat.codes.values\n",
    "\n",
    "num_users = data_with_emotions['uid_review'].nunique()\n",
    "num_items = data_with_emotions['anime_uid'].nunique()\n",
    "embedding_size = 20\n",
    "\n",
    "# Neural Collaborative Filtering model architecture\n",
    "user_input = Input(shape=(1,))\n",
    "item_input = Input(shape=(1,))\n",
    "content_input = Input(shape=(train_content_features_dense.shape[1],))\n",
    "\n",
    "user_embedding = Embedding(num_users, embedding_size, input_length=1)(user_input)\n",
    "item_embedding = Embedding(num_items, embedding_size, input_length=1)(item_input)\n",
    "user_vec = Flatten()(user_embedding)\n",
    "item_vec = Flatten()(item_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Model Architecture\n",
    "- Combine content-based and collaborative filtering features in a neural network\n",
    "\n",
    "- Evaluate model with MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "788/788 [==============================] - 11s 14ms/step - loss: 8.1819 - val_loss: 2.9647\n",
      "Epoch 2/5\n",
      "788/788 [==============================] - 20s 25ms/step - loss: 1.4987 - val_loss: 2.4187\n",
      "Epoch 3/5\n",
      "788/788 [==============================] - 42s 54ms/step - loss: 0.6136 - val_loss: 2.2877\n",
      "Epoch 4/5\n",
      "788/788 [==============================] - 25s 32ms/step - loss: 0.3200 - val_loss: 2.2362\n",
      "Epoch 5/5\n",
      "788/788 [==============================] - 22s 28ms/step - loss: 0.2471 - val_loss: 2.1727\n",
      "219/219 [==============================] - 2s 7ms/step - loss: 2.2556\n",
      "Mean Squared Error on Test Data: 2.255605697631836\n",
      "219/219 [==============================] - 1s 5ms/step\n",
      "Mean Absolute Error on Test Data: 1.0276234149932861\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import MeanAbsoluteError\n",
    "\n",
    "concatenated = concatenate([user_vec, item_vec, content_input])\n",
    "dense_layer_1 = Dense(128, activation='relu')(concatenated)\n",
    "output = Dense(1, activation='linear')(dense_layer_1)\n",
    "\n",
    "model_hybrid = Model(inputs=[user_input, item_input, content_input], outputs=output)\n",
    "model_hybrid.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "\n",
    "# Set a smaller batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Train the Model\n",
    "model_hybrid.fit(\n",
    "    [train_user_ids, train_item_ids, train_content_features_dense],\n",
    "    train_data['Overall'].values, \n",
    "    batch_size=batch_size, \n",
    "    epochs=5, \n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Evaluate the Model using MSE\n",
    "mse_loss = model_hybrid.evaluate(\n",
    "    [test_user_ids, test_item_ids, test_content_features_dense], \n",
    "    test_data['Overall'].values\n",
    ")\n",
    "print(f\"Mean Squared Error on Test Data: {mse_loss}\")\n",
    "\n",
    "# Predict on the test data\n",
    "test_predictions = model_hybrid.predict(\n",
    "    [test_user_ids, test_item_ids, test_content_features_dense]\n",
    ")\n",
    "\n",
    "# Calculate MAE\n",
    "mae_metric = MeanAbsoluteError()\n",
    "mae_metric.update_state(test_data['Overall'].values, test_predictions)\n",
    "mae = mae_metric.result().numpy()\n",
    "print(f\"Mean Absolute Error on Test Data: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process (Epochs 1-5)\n",
    "- Loss Over Epochs: The model's loss on the training set significantly decreases from 7.7444 in the first epoch to 0.2442 by the fifth epoch. This indicates that the model is learning and improving its predictions on the training data over each epoch.\n",
    "\n",
    "- Validation Loss: The validation loss also decreases over epochs, from 2.9592 to 2.2254, which is a positive sign. It suggests that the model is not just memorizing the training data but is also improving its performance on unseen data (validation set).\n",
    "\n",
    "### Final Evaluation\n",
    "- Test Loss: The model achieves a loss of 2.2711 on the test set. This is the mean squared error (MSE) between the predicted ratings and the actual ratings in the test data.\n",
    "\n",
    "\n",
    "### Interpretation\n",
    "- Model Learning: The decreasing trend in loss over epochs indicates that the model is effectively learning from the training data. However, the initial high loss suggests that the model may have started with predictions far from the actual values.\n",
    "\n",
    "- Overfitting Check: There is no significant divergence between training loss and validation loss, which is a good sign. A large gap would have indicated overfitting, where the model performs well on training data but poorly on unseen data.\n",
    "\n",
    "- Mean Squared Error: An MSE of 2.2711 on a rating scale (1-10) indicates that the model's predictions are, on average, about 1.51 units away from the actual ratings (since RMSE = sqrt(MSE) ≈ 1.51). The acceptability of this error depends on the context of your application. For a 1-10 scale, this deviation is moderate.\n",
    "\n",
    "### Overview\n",
    "\n",
    "In summary, the model is learning and improving its prediction accuracy over time, as evidenced by decreasing training and validation loss. The final MSE on the test set suggests moderate predictive accuracy, with potential areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the Model Architecture\n",
    "Modifying the architecture of the neural network by adding more layers, changing the number of neurons, and adding dropout layers to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Adjusted Neural Collaborative Filtering model architecture\n",
    "user_input = Input(shape=(1,))\n",
    "item_input = Input(shape=(1,))\n",
    "content_input = Input(shape=(train_content_features_dense.shape[1],))\n",
    "\n",
    "user_embedding = Embedding(num_users, embedding_size, input_length=1)(user_input)\n",
    "item_embedding = Embedding(num_items, embedding_size, input_length=1)(item_input)\n",
    "user_vec = Flatten()(user_embedding)\n",
    "item_vec = Flatten()(item_embedding)\n",
    "\n",
    "concatenated = concatenate([user_vec, item_vec, content_input])\n",
    "dense_layer_1 = Dense(256, activation='relu')(concatenated)\n",
    "dropout_1 = Dropout(0.3)(dense_layer_1)\n",
    "dense_layer_2 = Dense(128, activation='relu')(dropout_1)\n",
    "output = Dense(1, activation='linear')(dense_layer_2)\n",
    "\n",
    "model_hybrid = Model(inputs=[user_input, item_input, content_input], outputs=output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Adjust the learning rate\n",
    "model_hybrid.compile(optimizer=Adam(0.0005), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Adjusted Hybrid Model\n",
    "Increase epochs from 5 to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "788/788 [==============================] - 22s 28ms/step - loss: 6.7640 - val_loss: 3.3927\n",
      "Epoch 2/10\n",
      "788/788 [==============================] - 23s 30ms/step - loss: 2.1120 - val_loss: 2.6839\n",
      "Epoch 3/10\n",
      "788/788 [==============================] - 28s 36ms/step - loss: 0.9957 - val_loss: 2.4714\n",
      "Epoch 4/10\n",
      "788/788 [==============================] - 47s 59ms/step - loss: 0.6179 - val_loss: 2.4497\n",
      "Epoch 5/10\n",
      "788/788 [==============================] - 40s 50ms/step - loss: 0.4822 - val_loss: 2.3505\n",
      "Epoch 6/10\n",
      "788/788 [==============================] - 37s 47ms/step - loss: 0.4030 - val_loss: 2.2939\n",
      "Epoch 7/10\n",
      "788/788 [==============================] - 37s 47ms/step - loss: 0.3390 - val_loss: 2.1830\n",
      "Epoch 8/10\n",
      "788/788 [==============================] - 36s 46ms/step - loss: 0.2811 - val_loss: 2.1603\n",
      "Epoch 9/10\n",
      "788/788 [==============================] - 35s 45ms/step - loss: 0.2436 - val_loss: 2.0943\n",
      "Epoch 10/10\n",
      "788/788 [==============================] - 34s 44ms/step - loss: 0.2163 - val_loss: 2.0734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x15334c0a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hybrid.fit(\n",
    "    [train_user_ids, train_item_ids, train_content_features_dense],\n",
    "    train_data['Overall'].values, \n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Adjusted Hybrid Model with Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219/219 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9768970113680955"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = model_hybrid.predict(\n",
    "    [test_user_ids, test_item_ids, test_content_features_dense]\n",
    ")\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(test_data['Overall'].values, test_predictions)\n",
    "mae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Converting 'Birthday' to Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid_review</th>\n",
       "      <th>profile</th>\n",
       "      <th>anime_uid</th>\n",
       "      <th>scores</th>\n",
       "      <th>Overall</th>\n",
       "      <th>Story</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Sound</th>\n",
       "      <th>Character</th>\n",
       "      <th>Enjoyment</th>\n",
       "      <th>...</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>neutral</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27270</td>\n",
       "      <td>Elfsire</td>\n",
       "      <td>120</td>\n",
       "      <td>{'Overall': '8', 'Story': '8', 'Animation': '1...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010413</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.197999</td>\n",
       "      <td>0.028194</td>\n",
       "      <td>0.013339</td>\n",
       "      <td>0.689401</td>\n",
       "      <td>0.059908</td>\n",
       "      <td>2549</td>\n",
       "      <td>62</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228044</td>\n",
       "      <td>merryfistmas</td>\n",
       "      <td>227</td>\n",
       "      <td>{'Overall': '10', 'Story': '10', 'Animation': ...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186710</td>\n",
       "      <td>0.002664</td>\n",
       "      <td>0.069133</td>\n",
       "      <td>0.091217</td>\n",
       "      <td>0.139144</td>\n",
       "      <td>0.045635</td>\n",
       "      <td>0.465496</td>\n",
       "      <td>14674</td>\n",
       "      <td>111</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21744</td>\n",
       "      <td>ravenrage</td>\n",
       "      <td>3303</td>\n",
       "      <td>{'Overall': '6', 'Story': '3', 'Animation': '8...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007551</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.658758</td>\n",
       "      <td>0.077451</td>\n",
       "      <td>0.017352</td>\n",
       "      <td>0.235827</td>\n",
       "      <td>2087</td>\n",
       "      <td>887</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>248039</td>\n",
       "      <td>Tyrannicswine117</td>\n",
       "      <td>32615</td>\n",
       "      <td>{'Overall': '8', 'Story': '9', 'Animation': '8...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555890</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.173663</td>\n",
       "      <td>0.078683</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>0.014780</td>\n",
       "      <td>0.082877</td>\n",
       "      <td>16645</td>\n",
       "      <td>2249</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>215510</td>\n",
       "      <td>ThePunzalan</td>\n",
       "      <td>14813</td>\n",
       "      <td>{'Overall': '9', 'Story': '9', 'Animation': '9...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011294</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.938779</td>\n",
       "      <td>0.007481</td>\n",
       "      <td>0.031252</td>\n",
       "      <td>0.009808</td>\n",
       "      <td>13400</td>\n",
       "      <td>1652</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  uid_review           profile anime_uid  \\\n",
       "0      27270           Elfsire       120   \n",
       "1     228044      merryfistmas       227   \n",
       "2      21744         ravenrage      3303   \n",
       "3     248039  Tyrannicswine117     32615   \n",
       "4     215510       ThePunzalan     14813   \n",
       "\n",
       "                                              scores  Overall  Story  \\\n",
       "0  {'Overall': '8', 'Story': '8', 'Animation': '1...        8      8   \n",
       "1  {'Overall': '10', 'Story': '10', 'Animation': ...       10     10   \n",
       "2  {'Overall': '6', 'Story': '3', 'Animation': '8...        6      3   \n",
       "3  {'Overall': '8', 'Story': '9', 'Animation': '8...        8      9   \n",
       "4  {'Overall': '9', 'Story': '9', 'Animation': '9...        9      9   \n",
       "\n",
       "   Animation  Sound  Character  Enjoyment  ...     anger   disgust      fear  \\\n",
       "0         10      8         10          7  ...  0.010413  0.000746  0.197999   \n",
       "1         10     10         10         10  ...  0.186710  0.002664  0.069133   \n",
       "2          8      5          6          7  ...  0.007551  0.001111  0.001951   \n",
       "3          8      8          9          8  ...  0.555890  0.003051  0.173663   \n",
       "4          9      9         10          9  ...  0.011294  0.000683  0.000702   \n",
       "\n",
       "        joy   neutral   sadness  surprise   user  item   age  \n",
       "0  0.028194  0.013339  0.689401  0.059908   2549    62  32.0  \n",
       "1  0.091217  0.139144  0.045635  0.465496  14674   111  30.0  \n",
       "2  0.658758  0.077451  0.017352  0.235827   2087   887  33.0  \n",
       "3  0.078683  0.091056  0.014780  0.082877  16645  2249  26.0  \n",
       "4  0.938779  0.007481  0.031252  0.009808  13400  1652   NaN  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convert 'birthday' to datetime and then to age\n",
    "current_year = datetime.now().year\n",
    "data_with_emotions['age'] = data_with_emotions['birthday'].apply(lambda x: current_year - pd.to_datetime(x, errors='coerce').year)\n",
    "\n",
    "# Sanity check\n",
    "data_with_emotions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hybrid Model with Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF: Adjusted with parameters max_features and ngram_range.\n",
    "- Regularization: Added L1 and L2 regularization to the first dense layer.\n",
    "- Cross-Validation: Implemented with 5-fold cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Input, Flatten, concatenate, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1_l2\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Fill NaN values in 'synopsis' with empty string\n",
    "data_with_emotions['synopsis'].fillna('', inplace=True)\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = train_test_split(data_with_emotions, test_size=0.2, random_state=13)\n",
    "\n",
    "# Apply TF-IDF to the training set and transform the test set\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(1, 2))\n",
    "train_content_features = tfidf.fit_transform(train_data['synopsis'])\n",
    "test_content_features = tfidf.transform(test_data['synopsis'])\n",
    "\n",
    "# Convert sparse TF-IDF matrix to a dense format for both train and test sets\n",
    "train_content_features_dense = train_content_features.todense()\n",
    "test_content_features_dense = test_content_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare user and item interaction data for both train and test sets\n",
    "train_user_ids = train_data['uid_review'].astype('category').cat.codes.values\n",
    "train_item_ids = train_data['anime_uid'].astype('category').cat.codes.values\n",
    "test_user_ids = test_data['uid_review'].astype('category').cat.codes.values\n",
    "test_item_ids = test_data['anime_uid'].astype('category').cat.codes.values\n",
    "\n",
    "num_users = data_with_emotions['uid_review'].nunique()\n",
    "num_items = data_with_emotions['anime_uid'].nunique()\n",
    "embedding_size = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the model\n",
    "def create_model():\n",
    "    user_input = Input(shape=(1,))\n",
    "    item_input = Input(shape=(1,))\n",
    "    content_input = Input(shape=(train_content_features_dense.shape[1],))\n",
    "\n",
    "    user_embedding = Embedding(num_users, embedding_size, input_length=1)(user_input)\n",
    "    item_embedding = Embedding(num_items, embedding_size, input_length=1)(item_input)\n",
    "    user_vec = Flatten()(user_embedding)\n",
    "    item_vec = Flatten()(item_embedding)\n",
    "\n",
    "    concatenated = concatenate([user_vec, item_vec, content_input])\n",
    "    dense_layer_1 = Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(concatenated)\n",
    "    dropout_1 = Dropout(0.3)(dense_layer_1)\n",
    "    dense_layer_2 = Dense(128, activation='relu')(dropout_1)\n",
    "    output = Dense(1, activation='linear')(dense_layer_2)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input, content_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(0.0005), loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "630/630 [==============================] - 4s 5ms/step - loss: 18.9592 - val_loss: 4.7097\n",
      "Epoch 2/10\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.4315 - val_loss: 4.2943\n",
      "Epoch 3/10\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.8398 - val_loss: 3.8752\n",
      "Epoch 4/10\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 1.8742 - val_loss: 3.7399\n",
      "Epoch 5/10\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 1.4446 - val_loss: 3.2697\n",
      "Epoch 6/10\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 1.2434 - val_loss: 3.2069\n",
      "Epoch 7/10\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 1.1451 - val_loss: 3.2018\n",
      "Epoch 8/10\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 1.0839 - val_loss: 3.0934\n",
      "Epoch 9/10\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 1.0343 - val_loss: 3.0077\n",
      "Epoch 10/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.9793 - val_loss: 3.0701\n",
      "175/175 [==============================] - 0s 973us/step - loss: 3.1458\n",
      "Score for fold 1: 3.1457834243774414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "630/630 [==============================] - 4s 5ms/step - loss: 19.1326 - val_loss: 4.7790\n",
      "Epoch 2/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.3556 - val_loss: 4.3834\n",
      "Epoch 3/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7473 - val_loss: 3.9054\n",
      "Epoch 4/10\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 1.7810 - val_loss: 3.5295\n",
      "Epoch 5/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.3877 - val_loss: 3.4122\n",
      "Epoch 6/10\n",
      "630/630 [==============================] - 5s 7ms/step - loss: 1.1937 - val_loss: 3.2323\n",
      "Epoch 7/10\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 1.0974 - val_loss: 3.1743\n",
      "Epoch 8/10\n",
      "630/630 [==============================] - 5s 7ms/step - loss: 1.0576 - val_loss: 3.0950\n",
      "Epoch 9/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.0145 - val_loss: 3.0844\n",
      "Epoch 10/10\n",
      "630/630 [==============================] - 5s 8ms/step - loss: 0.9818 - val_loss: 3.0530\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 3.1871\n",
      "Score for fold 2: 3.187082290649414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 19.0967 - val_loss: 4.6045\n",
      "Epoch 2/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.4493 - val_loss: 4.3827\n",
      "Epoch 3/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9274 - val_loss: 4.1572\n",
      "Epoch 4/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.8982 - val_loss: 3.7194\n",
      "Epoch 5/10\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 1.4001 - val_loss: 3.4397\n",
      "Epoch 6/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.1982 - val_loss: 3.4108\n",
      "Epoch 7/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.1064 - val_loss: 3.2349\n",
      "Epoch 8/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.0499 - val_loss: 3.2202\n",
      "Epoch 9/10\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 1.0187 - val_loss: 3.3050\n",
      "Epoch 10/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.9960 - val_loss: 3.0934\n",
      "175/175 [==============================] - 0s 815us/step - loss: 3.0832\n",
      "Score for fold 3: 3.083169937133789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 19.0609 - val_loss: 4.7220\n",
      "Epoch 2/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.4271 - val_loss: 4.3346\n",
      "Epoch 3/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7649 - val_loss: 4.0568\n",
      "Epoch 4/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.7686 - val_loss: 3.4711\n",
      "Epoch 5/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.3472 - val_loss: 3.3818\n",
      "Epoch 6/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.1764 - val_loss: 3.2507\n",
      "Epoch 7/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.0904 - val_loss: 3.2084\n",
      "Epoch 8/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.0558 - val_loss: 3.0732\n",
      "Epoch 9/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.0110 - val_loss: 3.1570\n",
      "Epoch 10/10\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.9859 - val_loss: 3.0877\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 3.1210\n",
      "Score for fold 4: 3.1209983825683594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 19.3148 - val_loss: 4.7460\n",
      "Epoch 2/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.4447 - val_loss: 4.4281\n",
      "Epoch 3/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9425 - val_loss: 3.8823\n",
      "Epoch 4/10\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 1.8708 - val_loss: 3.5950\n",
      "Epoch 5/10\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 1.4131 - val_loss: 3.3857\n",
      "Epoch 6/10\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 1.2047 - val_loss: 3.2010\n",
      "Epoch 7/10\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 1.0981 - val_loss: 3.2211\n",
      "Epoch 8/10\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 1.0452 - val_loss: 3.1745\n",
      "Epoch 9/10\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 1.0188 - val_loss: 3.2555\n",
      "Epoch 10/10\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.9819 - val_loss: 3.0230\n",
      "175/175 [==============================] - 0s 1ms/step - loss: 3.0487\n",
      "Score for fold 5: 3.0487263202667236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define K-Fold Cross Validator\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "\n",
    "# Convert features and labels to numpy arrays for indexing\n",
    "train_user_ids = np.array(train_user_ids)\n",
    "train_item_ids = np.array(train_item_ids)\n",
    "train_content_features_dense = np.array(train_content_features_dense)\n",
    "train_labels = np.array(train_data['Overall'].values)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train_indices, test_indices in kfold.split(train_user_ids):\n",
    "    # Split data\n",
    "    X_train_user, X_test_user = train_user_ids[train_indices], train_user_ids[test_indices]\n",
    "    X_train_item, X_test_item = train_item_ids[train_indices], train_item_ids[test_indices]\n",
    "    X_train_content, X_test_content = train_content_features_dense[train_indices], train_content_features_dense[test_indices]\n",
    "    y_train, y_test = train_labels[train_indices], train_labels[test_indices]\n",
    "\n",
    "    # Create a new model instance\n",
    "    model = create_model()\n",
    "    \n",
    "    # Fit data to model\n",
    "    model.fit(\n",
    "        [X_train_user, X_train_item, X_train_content],\n",
    "        y_train, \n",
    "        batch_size=32, \n",
    "        epochs=10, \n",
    "        validation_split=0.1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    scores = model.evaluate(\n",
    "        [X_test_user, X_test_item, X_test_content],\n",
    "        y_test\n",
    "    )\n",
    "    print(f'Score for fold {fold_no}: {scores}')\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss and Validation Loss Over Epochs:\n",
    "- Decreasing Loss: For each fold, both the training loss and validation loss decrease over epochs, indicating that the model is learning and improving its predictions as training progresses.\n",
    "\n",
    "- Training Loss: Starts high in the first epoch (around 19) and drops significantly by the 10th epoch (around 1), showing that the model is effectively learning from the training data.\n",
    "\n",
    "- Validation Loss: Although it decreases, the validation loss doesn't decrease as sharply as the training loss. This is expected as the validation set is used to gauge the model's performance on unseen data.\n",
    "\n",
    "\n",
    "### Model Evaluation on Test Data:\n",
    "- Fold-wise Scores: The final loss scores on the test sets for each fold hover around 3. This suggests a consistent performance across different subsets of your data, which is a good sign of model stability.\n",
    "\n",
    "\n",
    "### Interpretation:\n",
    "- General Learning Trend: The model's ability to reduce loss over epochs indicates successful learning. However, the fact that the training loss is significantly lower than the validation loss could hint at overfitting, where the model performs well on training data but slightly worse on unseen data.\n",
    "\n",
    "- Model Consistency: The similar loss scores across different folds in cross-validation indicate that the model's performance is consistent regardless of the specific subset of data it's trained on.\n",
    "\n",
    "- Loss Magnitude: A loss of around 3, considering it's mean squared error, suggests the predictions are, on average, somewhat off from the actual values. The square root of this value gives an RMSE (Root Mean Squared Error) of approximately 1.73, which can be interpreted as the average deviation from the actual values in your rating prediction task. On a scale of 1-10, this deviation is moderate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender\n",
       "Male          26017\n",
       "Female         8489\n",
       "Non-Binary      482\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_emotions['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'age' into age brackets\n",
    "def age_bracket(age):\n",
    "    if age < 18:\n",
    "        return 'Under 18'\n",
    "    elif 18 <= age < 25:\n",
    "        return '18-24'\n",
    "    elif 25 <= age < 35:\n",
    "        return '25-34'\n",
    "    elif 35 <= age < 45:\n",
    "        return '35-44'\n",
    "    elif 45 <= age < 55:\n",
    "        return '45-54'\n",
    "    else:\n",
    "        return '55+'\n",
    "\n",
    "data_with_emotions['age_bracket'] = data_with_emotions['age'].apply(age_bracket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in 'synopsis' with empty string\n",
    "data_with_emotions['synopsis'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hybrid with Gender/Age Brackets\n",
    "Regression model predicts how a user might rate an anime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is a comprehensive process for training a neural network model on a dataset with text and categorical features. Let's break down each part of the code and then interpret the model's results:\n",
    "\n",
    "Code Explanation\n",
    "Text Preprocessing Pipeline:\n",
    "\n",
    "A pipeline is defined using TfidfVectorizer to transform the 'synopsis' text data into a TF-IDF matrix. It removes English stopwords, limits the number of features to 1000, and considers unigrams and bigrams.\n",
    "Overall Preprocessor:\n",
    "\n",
    "ColumnTransformer is used to apply different transformations to different columns: TF-IDF vectorization to 'synopsis', one-hot encoding to 'age_bracket', and 'gender' columns.\n",
    "Data Splitting:\n",
    "\n",
    "The dataset is split into training and test sets (80-20 split). The training set is further split into a smaller training set and a validation set (90-10 split).\n",
    "Data Transformation:\n",
    "\n",
    "The preprocessor is fitted on the training data and used to transform the training, validation, and test sets. The transformed data is converted to dense format.\n",
    "ID Preparation:\n",
    "\n",
    "User and item IDs are extracted from the training, validation, and test datasets. These are categorical variables converted to numerical codes.\n",
    "Model Definition:\n",
    "\n",
    "A function create_model is defined to create a neural network model. The model uses embedding layers for user and item IDs, concatenates these with the additional preprocessed features, and passes them through dense layers.\n",
    "Model Creation and Compilation:\n",
    "\n",
    "The model is created with the appropriate input shape and compiled with the Adam optimizer and mean squared error loss function.\n",
    "Model Training:\n",
    "\n",
    "The model is trained on the training data, with the validation data used to monitor performance during training.\n",
    "Model Evaluation:\n",
    "\n",
    "The model's performance is evaluated on the test set using Mean Absolute Error (MAE) as the metric.\n",
    "Optimizer Warning:\n",
    "\n",
    "A warning indicates that the Adam optimizer version being used may run slowly on M1/M2 Macs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Flatten, concatenate, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Define text preprocessing pipeline\n",
    "text_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(1, 2)))\n",
    "])\n",
    "\n",
    "# Define overall preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_pipeline, 'synopsis'),\n",
    "        ('onehot_age', OneHotEncoder(), ['age_bracket']),\n",
    "        ('onehot_gender', OneHotEncoder(), ['gender'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = train_test_split(data_with_emotions, test_size=0.2, random_state=13)\n",
    "\n",
    "# Further split training data into training and 10% validation set\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=13)\n",
    "\n",
    "# Apply preprocessing\n",
    "train_transformed = preprocessor.fit_transform(train_data).todense()\n",
    "val_transformed = preprocessor.transform(val_data).todense()\n",
    "test_transformed = preprocessor.transform(test_data).todense()\n",
    "\n",
    "# Prepare user and item IDs for training, validation, and testing\n",
    "train_user_ids = train_data['uid_review'].astype('category').cat.codes.values\n",
    "train_item_ids = train_data['anime_uid'].astype('category').cat.codes.values\n",
    "\n",
    "val_user_ids = val_data['uid_review'].astype('category').cat.codes.values\n",
    "val_item_ids = val_data['anime_uid'].astype('category').cat.codes.values\n",
    "\n",
    "test_user_ids = test_data['uid_review'].astype('category').cat.codes.values\n",
    "test_item_ids = test_data['anime_uid'].astype('category').cat.codes.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def create_model(num_users, num_items, input_shape):\n",
    "    user_input = Input(shape=(1,))\n",
    "    item_input = Input(shape=(1,))\n",
    "    additional_input = Input(shape=(input_shape,))\n",
    "\n",
    "    user_embedding = Embedding(num_users, 20, input_length=1)(user_input)\n",
    "    item_embedding = Embedding(num_items, 20, input_length=1)(item_input)\n",
    "    user_vec = Flatten()(user_embedding)\n",
    "    item_vec = Flatten()(item_embedding)\n",
    "\n",
    "    concatenated = concatenate([user_vec, item_vec, additional_input])\n",
    "    dense_layer = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(concatenated)\n",
    "    output = Dense(1, activation='linear')(dense_layer)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input, additional_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(0.0005), loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "788/788 [==============================] - 6s 7ms/step - loss: 11.7429 - val_loss: 4.2300\n",
      "Epoch 2/10\n",
      "788/788 [==============================] - 5s 7ms/step - loss: 2.8092 - val_loss: 3.0669\n",
      "Epoch 3/10\n",
      "788/788 [==============================] - 4s 6ms/step - loss: 1.2951 - val_loss: 2.6411\n",
      "Epoch 4/10\n",
      "788/788 [==============================] - 4s 5ms/step - loss: 0.6979 - val_loss: 2.5108\n",
      "Epoch 5/10\n",
      "788/788 [==============================] - 4s 5ms/step - loss: 0.4569 - val_loss: 2.4548\n",
      "Epoch 6/10\n",
      "788/788 [==============================] - 5s 6ms/step - loss: 0.3332 - val_loss: 2.3901\n",
      "Epoch 7/10\n",
      "788/788 [==============================] - 5s 7ms/step - loss: 0.2598 - val_loss: 2.3425\n",
      "Epoch 8/10\n",
      "788/788 [==============================] - 5s 7ms/step - loss: 0.2102 - val_loss: 2.2923\n",
      "Epoch 9/10\n",
      "788/788 [==============================] - 5s 6ms/step - loss: 0.1739 - val_loss: 2.2789\n",
      "Epoch 10/10\n",
      "788/788 [==============================] - 4s 6ms/step - loss: 0.1485 - val_loss: 2.2967\n",
      "219/219 [==============================] - 0s 633us/step\n",
      "Mean Absolute Error: 1.0291059274747052\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "input_shape = train_transformed.shape[1]\n",
    "rating_predictor = create_model(len(train_user_ids), len(train_item_ids), input_shape)\n",
    "\n",
    "# Prepare inputs for the model\n",
    "train_inputs = [train_user_ids, train_item_ids, train_transformed]\n",
    "val_inputs = [val_user_ids, val_item_ids, val_transformed]\n",
    "test_inputs = [test_user_ids, test_item_ids, test_transformed]\n",
    "\n",
    "# Train the model\n",
    "rating_predictor.fit(\n",
    "    train_inputs,\n",
    "    train_data['Overall'].values,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(val_inputs, val_data['Overall'].values)\n",
    ")\n",
    "\n",
    "# Predict and evaluate using MAE\n",
    "predictions = rating_predictor.predict(test_inputs)\n",
    "mae = mean_absolute_error(test_data['Overall'].values, predictions)\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results Interpretation\n",
    "\n",
    "\n",
    "### Training and Validation Loss:\n",
    "\n",
    "The training loss decreases from 14.2328 to 0.1535 over 10 epochs, indicating that the model is learning effectively from the training data.\n",
    "The validation loss also decreases, from 4.4708 to 2.2695, suggesting that the model generalizes well to unseen data.\n",
    "\n",
    "\n",
    "### Evaluation - Mean Absolute Error (MAE)\n",
    "\n",
    "The MAE on the test set is 1.0149. This metric indicates the average absolute difference between the predicted and actual values. Given the scale of the 'Overall' ratings (1-10), an MAE of around 1 suggests that the model's predictions are, on average, about one rating point away from the actual values.\n",
    "\n",
    "\n",
    "### Features\n",
    "\n",
    "- The model integrates textual features from the 'synopsis' (processed through TF-IDF) and categorical features from 'age_bracket' and 'gender' (one-hot encoded).\n",
    "\n",
    "- The text features capture the semantic content of the synopses, while the categorical features provide demographic context.\n",
    "\n",
    "\n",
    "### Effectiveness\n",
    "\n",
    "The combination of these features, along with user and item IDs, gives the model a nuanced view of the interactions, likely contributing to its predictive performance.\n",
    "\n",
    "### Conclusion\n",
    "The model demonstrates effective learning and good generalization, as evidenced by decreasing training and validation loss. The MAE of 1.0149 indicates reasonable accuracy, considering the complexity of predicting precise rating values. The integration of text and categorical features, along with user and item embeddings, appears to provide a robust representation for the prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Models for Playlist Generation\n",
    "\n",
    "User Input for Mood:\n",
    "- Create an interface (e.g., a web form) where users can input their mood, age, and gender.\n",
    "\n",
    "Mood-Based Anime Filtering:\n",
    "- Use the mood classification model to filter anime that matches the selected mood.\n",
    "\n",
    "Rating Predictions:\n",
    "- Use the rating prediction model to estimate how the user would rate these mood-matched anime.\n",
    "\n",
    "Generate and Sort Playlist:\n",
    "- Sort these anime based on the predicted ratings to create a playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Recommendations Models\n",
    "Combining collaborative and content-based filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendation(user_id, content_rec, collaborative_rec, mood=None):\n",
    "    # Hybrid model combining content and collaborative filtering\n",
    "    hybrid_rec = content_rec * 0.5 + collaborative_rec * 0.5\n",
    "\n",
    "    # If mood is specified, filter or re-rank recommendations\n",
    "    if mood:\n",
    "        hybrid_rec = filter_or_rank_by_mood(hybrid_rec, mood)\n",
    "\n",
    "    return hybrid_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendation(user_id, mood=None):\n",
    "    # Get content-based recommendations\n",
    "    content_based_rec = content_based_model(user_id)\n",
    "\n",
    "    # Get collaborative filtering recommendations\n",
    "    collaborative_rec = collaborative_model(user_id)\n",
    "\n",
    "    # Get mood-based recommendations if a mood is specified\n",
    "    if mood:\n",
    "        mood_based_rec = mood_based_model(user_id, mood)\n",
    "    else:\n",
    "        mood_based_rec = {}\n",
    "\n",
    "    # Combine recommendations\n",
    "    # This is a simple average. Consider using more complex methods\n",
    "    combined_rec = average(content_based_rec, collaborative_rec, mood_based_rec)\n",
    "\n",
    "    return combined_rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalization and Playlist Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_playlist(user_id, mood=None):\n",
    "    content_rec = get_content_based_recommendations(user_id)\n",
    "    collaborative_rec = get_collaborative_recommendations(user_id)\n",
    "    \n",
    "    # Hybrid recommendations\n",
    "    recommendations = hybrid_recommendation(user_id, content_rec, collaborative_rec, mood)\n",
    "\n",
    "    # Generate playlist based on recommendations\n",
    "    playlist = create_playlist(recommendations)\n",
    "    return playlist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-interface Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: joblib in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (1.3.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rating_predictor.joblib']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Save models\n",
    "dump(mood_classifier, 'mood_classifier.joblib')\n",
    "dump(rating_predictor, 'rating_predictor.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function for mood classifier pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_mood_classifier(X, categorical_feature, numerical_feature):\n",
    "    # Define the categorical and numerical transformers\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean'))\n",
    "    ])\n",
    "\n",
    "    # Combine preprocessing for different types of features\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_feature),\n",
    "            ('num', numerical_transformer, numerical_feature)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Apply the preprocessing to the input data\n",
    "    X_transformed = preprocessor.transform(X)\n",
    "\n",
    "    return X_transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create function for ratings predictions pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_for_rating_predictor(data, text_pipeline, preprocessor):\n",
    "    # Extract user and item IDs\n",
    "    user_ids = data['uid_review'].astype('category').cat.codes.values\n",
    "    item_ids = data['anime_uid'].astype('category').cat.codes.values\n",
    "\n",
    "    # Apply text preprocessing\n",
    "    text_features = data['synopsis']\n",
    "    text_transformed = text_pipeline.transform(text_features).todense()\n",
    "\n",
    "    # Apply one-hot encoding and concatenate with text features\n",
    "    categorical_features = data[['age_bracket', 'gender']]\n",
    "    categorical_transformed = preprocessor.transform(categorical_features).todense()\n",
    "\n",
    "    # Combine all features\n",
    "    all_features = np.concatenate([text_transformed, categorical_transformed], axis=1)\n",
    "\n",
    "    return user_ids, item_ids, all_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to filter anime by predicted mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_anime_by_mood(predicted_mood):\n",
    "    # Filter the dataset for anime that matches the predicted mood\n",
    "    return data_with_emotions[data_with_emotions['mood_label'] == predicted_mood]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to sort anime based on predicted ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_anime_by_ratings(filtered_anime, predicted_ratings):\n",
    "    # Add the predicted ratings to the filtered anime dataset\n",
    "    filtered_anime['predicted_rating'] = predicted_ratings\n",
    "    \n",
    "    # Sort the anime by predicted ratings in descending order\n",
    "    sorted_anime = filtered_anime.sort_values(by='predicted_rating', ascending=False)\n",
    "    \n",
    "    # Return the sorted anime\n",
    "    return sorted_anime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Flask\n",
      "  Downloading flask-3.0.0-py3-none-any.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 4.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting itsdangerous>=2.1.2\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from Flask) (3.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from Flask) (6.8.0)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from Flask) (3.0.1)\n",
      "Collecting blinker>=1.6.2\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from Flask) (8.1.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=3.6.0->Flask) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kariprimiano/Library/Python/3.9/lib/python/site-packages (from Jinja2>=3.1.2->Flask) (2.1.3)\n",
      "Installing collected packages: itsdangerous, blinker, Flask\n",
      "Successfully installed Flask-3.0.0 blinker-1.7.0 itsdangerous-2.1.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5001\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug: * Restarting with stat\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kariprimiano/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/kariprimiano/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1076, in launch_instance\n",
      "    app.initialize(argv)\n",
      "  File \"/Users/kariprimiano/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 118, in inner\n",
      "    return method(app, *args, **kwargs)\n",
      "  File \"/Users/kariprimiano/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 692, in initialize\n",
      "    self.init_sockets()\n",
      "  File \"/Users/kariprimiano/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 331, in init_sockets\n",
      "    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n",
      "  File \"/Users/kariprimiano/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 253, in _bind_socket\n",
      "    return self._try_bind_socket(s, port)\n",
      "  File \"/Users/kariprimiano/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 229, in _try_bind_socket\n",
      "    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n",
      "  File \"/Users/kariprimiano/Library/Python/3.9/lib/python/site-packages/zmq/sugar/socket.py\", line 302, in bind\n",
      "    super().bind(addr)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 564, in zmq.backend.cython.socket.Socket.bind\n",
      "  File \"zmq/backend/cython/checkrc.pxd\", line 28, in zmq.backend.cython.checkrc._check_rc\n",
      "zmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:9007')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, render_template\n",
    "from joblib import load\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the trained models\n",
    "mood_classifier = load('mood_classifier.joblib')\n",
    "rating_predictor = load('rating_predictor.joblib')\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'POST':\n",
    "        # Process user input\n",
    "        user_mood = request.form.get('mood')\n",
    "        user_age = request.form.get('age')\n",
    "        # Convert age to a numeric value\n",
    "        try:\n",
    "            user_age = int(user_age)\n",
    "        except ValueError:\n",
    "            # Handle cases where age is not a number\n",
    "            user_age = train_data['age'].median()\n",
    "                \n",
    "        user_gender = request.form.get('gender')\n",
    "        # Validate user_gender input\n",
    "        if user_gender not in ['Male', 'Female', 'Non-Binary']:\n",
    "            user_gender = train_data['gender'].median()\n",
    "        \n",
    "        # Generate playlist\n",
    "        playlist = generate_playlist(user_mood, user_age, user_gender)\n",
    "\n",
    "        return render_template('playlist.html', playlist=playlist)\n",
    "\n",
    "    return render_template('index.html')\n",
    "\n",
    "def generate_playlist(mood, age, gender):\n",
    "    # Preprocess input for the mood classification model\n",
    "    mood_input = preprocess_for_mood_classifier(mood, age, gender)\n",
    "    predicted_mood = mood_classifier.predict(mood_input)\n",
    "\n",
    "    # Filter anime based on predicted mood\n",
    "    filtered_anime = filter_anime_by_mood(predicted_mood)\n",
    "\n",
    "    # Preprocess the filtered anime for the rating prediction model\n",
    "    rating_input = preprocess_for_rating_predictor(filtered_anime, age, gender)\n",
    "    predicted_ratings = rating_predictor.predict(rating_input)\n",
    "\n",
    "    # Combine predictions with anime details and sort by ratings\n",
    "    sorted_playlist = sort_anime_by_ratings(predicted_ratings)\n",
    "\n",
    "    # Return the sorted playlist as a list of recommended anime titles\n",
    "    return sorted_playlist\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Considerations\n",
    "\n",
    "- img_url, link_x: If you have the capability to process image data, the anime's poster or image could be used for mood prediction using advanced techniques like Convolutional Neural Networks (CNNs). However, this is more complex and may require substantial computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
