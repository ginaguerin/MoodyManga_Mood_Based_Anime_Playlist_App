{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoodyManga\n",
    "### A New Wave Recommendation System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\" style=\"border: 2px solid black;\">\n",
    "    <img src=\"images/custom_anime_header2.png\" alt=\"MoodyManga\" width=\"600\" height=\"300\">\n",
    "</div>\n",
    "<div align=\"left\">\n",
    "    <font size=\"1\">Image by DALL-E</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "An anime playlist generator that not only uses traditional collaborative and content-based recommendation models, but incorporates Natural Language Processing mood-based classifications as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Plan\n",
    "MoodyManga is a personalized application that understands the feelings behind each story and recommends a match based on your mood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "MyAnimeList:\n",
    "- [Animes](https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews/?select=animes.csv)\n",
    "    - Contains list of anime, with title, genre, rank, populatiry, review scores, air date, episodes and more.\n",
    "- [Profiles](https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews/?select=profiles.csv)\n",
    "    - Contains information about users-info including anime, username, birth date, gender, and favorite animes list.\n",
    "- [Reviews](https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews/?select=reviews.csv)\n",
    "    - Contains information about comprehensive user review scores.\n",
    "\n",
    "TMDB:\n",
    "- [Anime](https://www.themoviedb.org/list/7102-anime)\n",
    "    - Contains information about anime title and review scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Features\n",
    "- uid_review: Unique review ID \n",
    "- profile: Unique profile username\n",
    "- anime_uid: Unique anime ID that user has rated\n",
    "- scores: Dictionary comprehensive review scores 1-10 scale\n",
    "- Overall: Overall review score 1-10 scale\n",
    "- Story: Store review score 1-10 scale \n",
    "- Animation: Animation review score 1-10 scale \n",
    "- Sound: Sound review score 1-10 scale \n",
    "- Character: Character review score 1-10 scale \n",
    "- Enjoyment: Enjoyment review score 1-10 scale\n",
    "- title: Title\n",
    "- synopsis: Plot summary\n",
    "- genre: Comma separated list of genres\n",
    "- aired: Date aired\n",
    "- episodes: Number of episodes\n",
    "- members: Number of community members in anime group\n",
    "- popularity: Popularity count of anime\n",
    "- ranked: Ranked score based on reviews and popularity\n",
    "- img_url: Anime title image URL\n",
    "- gender: User gender\n",
    "- birthday: User birth date\n",
    "- favorites_anime: User list of favorite anime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "- Anime often categorized as \"animation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "See 'Data_Notebook.ipynb' for full details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Input, concatenate, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from keras.regularizers import l1_l2\n",
    "from keras import backend as K\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.metrics import MeanAbsoluteError\n",
    "import scipy.sparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Input, concatenate, Dense\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting master data csv from zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = \"zipped_data/master_anime_data.csv.zip\"\n",
    "csv_file_name = \"zipped_data/master_anime_data.csv\"\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "    # Check if the CSV file exists within the zip archive\n",
    "    if csv_file_name in zip_file.namelist():\n",
    "        # Extract the CSV file to a temporary location\n",
    "        zip_file.extract(csv_file_name, path=\"path/to/extracted/csv_file\")\n",
    "\n",
    "        # Specify the path to the extracted CSV file\n",
    "        extracted_csv_path = f\"path/to/extracted/csv_file/{csv_file_name}\"\n",
    "\n",
    "        # Use pandas to read the CSV data into a DataFrame\n",
    "        df = pd.read_csv(extracted_csv_path)\n",
    "\n",
    "        # Now you can work with the DataFrame (df) containing your data\n",
    "    else:\n",
    "        print(f\"The file '{csv_file_name}' does not exist in the zip archive.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in master dataset to sample\n",
    "# master_data = pd.read_csv('zipped_data/master_anime_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling data for running through models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset of your data for processing\n",
    "#sampled_data = master_data.sample(frac=0.1, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = pd.read_csv('zipped_data/sample_dataset.csv')\n",
    "sampled_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; border: 2px solid black;\">\n",
    "    <!-- Add padding to the div element -->\n",
    "    <div style=\"padding: 20px 0;\">\n",
    "        <img src=\"images/K_G_Logo.png\" alt=\"K&G Logo\" width=\"150\" height=\"150\">\n",
    "    </div>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <font size=\"1\">Image by DALL-E</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Super-users\n",
    "Using percentiles to define super-users in the top 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Master data super users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of interactions per user\n",
    "interaction_counts = master_data.groupby('profile').size()\n",
    "\n",
    "# Using percentiles to define super-users\n",
    "percentile_threshold = 95  # Top 5% of users\n",
    "threshold = interaction_counts.quantile(percentile_threshold / 100)\n",
    "super_users = interaction_counts[interaction_counts > threshold].index.tolist()\n",
    "\n",
    "print(f\"Threshold for super-users (top {100 - percentile_threshold}%): {threshold:.2f}\")\n",
    "print(f\"Identified {len(super_users)} super-users.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample data super users - subset of ~34K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of interactions per user\n",
    "interaction_counts = sampled_data.groupby('profile').size()\n",
    "\n",
    "# Using percentiles to define super-users\n",
    "percentile_threshold = 95  # Top 5% of users\n",
    "threshold = interaction_counts.quantile(percentile_threshold / 100)\n",
    "super_users = interaction_counts[interaction_counts > threshold].index.tolist()\n",
    "\n",
    "print(f\"Threshold for super-users (top {100 - percentile_threshold}%): {threshold:.2f}\")\n",
    "print(f\"Identified {len(super_users)} super-users.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of super-users is not large enough to significantly impact EDA or model results. Therefore, we will continue without separating super-users from dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring relationship between Ratings and Popularity by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplots\n",
    "sns.relplot(y='score',x='popularity',data=sampled_data,col='gender',hue='gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Converting 'Birthday' to Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'birthday' to datetime and then to age\n",
    "current_year = datetime.now().year\n",
    "sampled_data['age'] = sampled_data['birthday'].apply(lambda x: current_year - pd.to_datetime(x, errors='coerce').year)\n",
    "\n",
    "# Sanity check\n",
    "sampled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring relationship between Ratings and Age by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplots\n",
    "sns.relplot(y='score',x='age',data=sampled_data,col='gender',hue='gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Distribution Visualizations\n",
    "Using histograms, we'll plot the distributions of key numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fig size\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Cfreate subplots for each column\n",
    "\n",
    "# Popularity\n",
    "plt.subplot(2, 3, 1)  # 2 rows, 3 columns, 1st subplot\n",
    "sns.histplot(sampled_data['popularity'], bins=20, kde=True)\n",
    "plt.title('Popularity Distribution')\n",
    "\n",
    "# Ranked\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.histplot(sampled_data['ranked'], bins=20, kde=True)\n",
    "plt.title('Ranked Distribution')\n",
    "\n",
    "# Overall\n",
    "plt.subplot(2, 3, 3)\n",
    "sns.histplot(sampled_data['Overall'], bins=20, kde=True)\n",
    "plt.title('Overall Distribution')\n",
    "\n",
    "# Gender\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.countplot(sampled_data['gender'])\n",
    "plt.title('Gender Distribution')\n",
    "\n",
    "# Age\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.histplot(sampled_data['age'], bins=20, kde=True)\n",
    "plt.title('Age Distribution')\n",
    "\n",
    "\n",
    "# Adjust layout spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; border: 2px solid black;\">\n",
    "    <!-- Add padding to the div element -->\n",
    "    <div style=\"padding: 20px 0;\">\n",
    "        <img src=\"images/K_G_Logo.png\" alt=\"K&G Logo\" width=\"150\" height=\"150\">\n",
    "    </div>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <font size=\"1\">Image by DALL-E</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maching Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Classification in Texts with Natural Language Processing\n",
    "Processing emotion classification in 'synopsis' data, leveraging NLP techniques and a pre-trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download nltk stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the emotion classification pipeline\n",
    "model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "emotion_classifier = pipeline('text-classification', model=model_name, return_all_scores=True)\n",
    "\n",
    "# Set English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function for advanced text cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Function to classify emotions of a text\n",
    "def classify_emotions(text):\n",
    "    cleaned_text = clean_text(text)  # Clean the text using clean_text function\n",
    "    predictions = emotion_classifier(cleaned_text)\n",
    "    return {emotion['label']: emotion['score'] for emotion in predictions[0]}\n",
    "\n",
    "# Apply the model to the 'synopsis' column\n",
    "sampled_data['emotion_scores'] = sampled_data['synopsis'].astype(str).apply(classify_emotions)\n",
    "\n",
    "# Inspect results\n",
    "sampled_data[['synopsis', 'emotion_scores']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data['emotion_scores'].value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding 'emotion_scores' into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the emotion_scores column needs to be parsed from string to dictionary\n",
    "if isinstance(sampled_data['emotion_scores'].iloc[0], str):\n",
    "    sampled_data['emotion_scores'] = sampled_data['emotion_scores'].apply(ast.literal_eval)\n",
    "\n",
    "# Expand the emotion_scores column into separate columns\n",
    "emotion_scores = sampled_data['emotion_scores'].apply(pd.Series)\n",
    "\n",
    "# Join the expanded emotion scores dataframe with the original data\n",
    "data_with_emotions = sampled_data.join(emotion_scores)\n",
    "\n",
    "# Display the first few rows to verify the expansion\n",
    "data_with_emotions.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking genres with highest emotion scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage for each emotion\n",
    "emotion_columns = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "data_with_emotions[emotion_columns] = data_with_emotions[emotion_columns].div(data_with_emotions[emotion_columns].sum(axis=1), axis=0)\n",
    "\n",
    "# Group by genre and calculate mean percentage for each emotion\n",
    "genre_emotion_means = data_with_emotions.groupby('genre')[emotion_columns].mean()\n",
    "\n",
    "# For each emotion, find the genre with the highest percentage\n",
    "highest_emotion_genres = genre_emotion_means.idxmax()\n",
    "\n",
    "print(highest_emotion_genres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving new dataset with emotion scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new dataset\n",
    "data_with_emotions.to_csv('zipped_data/data_with_emotions.csv')\n",
    "data_with_emotions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; border: 2px solid black;\">\n",
    "    <!-- Add padding to the div element -->\n",
    "    <div style=\"padding: 20px 0;\">\n",
    "        <img src=\"images/K_G_Logo.png\" alt=\"K&G Logo\" width=\"150\" height=\"150\">\n",
    "    </div>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <font size=\"1\">Image by DALL-E</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classification Model\n",
    "- Classification Model for Mood Categorization\n",
    "- Using anime_uid, title, genre, and popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-Based Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mood_label(row):\n",
    "    if row['joy'] > 0.5:\n",
    "        return 'Happy'\n",
    "    elif row['sadness'] > 0.5:\n",
    "        return 'Sad'\n",
    "    elif row['anger'] > 0.5:\n",
    "        return 'Angry'\n",
    "    elif row['disgust'] > 0.5:\n",
    "        return 'Disgust'\n",
    "    elif row['fear'] > 0.5:\n",
    "        return 'Fear'\n",
    "    elif row['surprise'] > 0.5:\n",
    "        return 'Excitement'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "data_with_emotions['mood_label'] = data_with_emotions.apply(create_mood_label, axis=1)\n",
    "data_with_emotions['mood_label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data - Features and Labels\n",
    "X = data_with_emotions[['uid_anime', 'title', 'genre', 'popularity']]\n",
    "y = data_with_emotions['mood_label']\n",
    "\n",
    "numerical_feature = ['uid_anime', 'popularity']\n",
    "categorical_feature = ['title', 'genre']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "\n",
    "# Categorical feature preprocessing\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Numerical feature preprocessing with missing value handling\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing for different types of features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_feature),\n",
    "        ('num', numerical_transformer, numerical_feature)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "# We need to apply preprocessing to train and test sets separately\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "classifier = RandomForestClassifier(random_state=13)\n",
    "\n",
    "# Train the classifier with transformed training data\n",
    "classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Perform cross-validation\n",
    "# Note: For cross-validation, you need to preprocess the entire dataset\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "cross_val_scores = cross_val_score(classifier, X_transformed, y, cv=5)\n",
    "print(f'Cross-Validation Scores: {cross_val_scores.mean():.2f}')\n",
    "\n",
    "# Make predictions on the transformed test set\n",
    "y_pred = classifier.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy on Test Data: {accuracy:.2f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:\\n', cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Classification Model Using Popularity & Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data - Features and Labels\n",
    "X = data_with_emotions[['genre', 'popularity']]\n",
    "y = data_with_emotions['mood_label']\n",
    "\n",
    "numerical_feature = ['popularity']\n",
    "categorical_feature = ['genre']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "\n",
    "# Categorical feature preprocessing\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Numerical feature preprocessing with missing value handling\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing for different types of features\n",
    "rfc_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_feature),\n",
    "        ('num', numerical_transformer, numerical_feature)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_transformed = rfc_preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = rfc_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Random Forest Classifier Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the preprocessor\n",
    "joblib.dump(rfc_preprocessor, 'rfc_preprocessor.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Saving with pickle\n",
    "with open('rfc_preprocessor.pkl', 'wb') as file:\n",
    "    pickle.dump(rfc_preprocessor, file)\n",
    "\n",
    "# Loading with pickle\n",
    "with open('rfc_preprocessor.pkl', 'rb') as file:\n",
    "    rfc_preprocessor = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Tuned Random Forest Classifier\n",
    "To be used in the user-interface app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "mood_classifier = RandomForestClassifier(random_state=13)\n",
    "\n",
    "# Train the classifier with transformed training data\n",
    "mood_classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Perform cross-validation\n",
    "# Note: For cross-validation, you need to preprocess the entire dataset\n",
    "X_transformed = rfc_preprocessor.fit_transform(X)\n",
    "cross_val_scores = cross_val_score(mood_classifier, X_transformed, y, cv=5)\n",
    "print(f'Cross-Validation Scores: {cross_val_scores.mean():.2f}')\n",
    "\n",
    "# Make predictions on the transformed test set\n",
    "y_pred = mood_classifier.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy on Test Data: {accuracy:.2f}')\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:\\n', cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Random Forest Classifier Model\n",
    "To be used in the user-interface app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "joblib.dump(mood_classifier, 'mood_classifier.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Saving with pickle\n",
    "with open('mood_classifier.pkl', 'wb') as file:\n",
    "    pickle.dump(mood_classifier, file)\n",
    "\n",
    "# Loading with pickle\n",
    "with open('mood_classifier.pkl', 'rb') as file:\n",
    "    mood_classifier = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "### Model Performance\n",
    "The average cross-validation score is 0.98, which is extremely high. This score indicates the model's ability to generalize well over different subsets of the dataset. A high cross-validation score is desirable as it suggests consistent performance across different partitions of the data.\n",
    "\n",
    "- The model achieved an accuracy of 0.98 on the test data, which aligns with the cross-validation score. This high accuracy suggests that the model is very effective in classifying the mood correctly.\n",
    "\n",
    "\n",
    "- The Confusion Matrix shows a high true positive rate and a low false positive rate for most classes.\n",
    "\n",
    "\n",
    "### Feature Used in Model\n",
    "The high accuracy and cross-validation scores indicate that the model performs exceptionally well in mood classification. It suggests that Popularity and Genre are highly predictive of the mood.\n",
    "\n",
    "### Conclusion\n",
    "In summary, your RandomForestClassifier model shows excellent performance in mood classification with high accuracy and generalization capability. The features used, processed by the rfc_preprocessor, play a significant role in this high performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; border: 2px solid black;\">\n",
    "    <!-- Add padding to the div element -->\n",
    "    <div style=\"padding: 20px 0;\">\n",
    "        <img src=\"images/K_G_Logo.png\" alt=\"K&G Logo\" width=\"150\" height=\"150\">\n",
    "    </div>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <font size=\"1\">Image by DALL-E</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Deep Learning Models\n",
    "Combine features from both content-based and collaborative filtering in a deep learning architecture, allowing the model to learn complex interactions between content and user preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Based Filtering\n",
    "Feature Extraction from Synopsis using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in 'synopsis' with empty string\n",
    "data_with_emotions['synopsis'].fillna('', inplace=True)\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = train_test_split(data_with_emotions, test_size=0.2, random_state=13)\n",
    "\n",
    "# Apply TF-IDF to the training set and transform the test set\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "train_content_features = tfidf.fit_transform(train_data['synopsis'])\n",
    "test_content_features = tfidf.transform(test_data['synopsis'])\n",
    "\n",
    "# Convert sparse TF-IDF matrix to a dense format for both train and test sets\n",
    "train_content_features_dense = train_content_features.todense()\n",
    "test_content_features_dense = test_content_features.todense()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Collaborative Filtering Data\n",
    "Prepare user-item interaction data and create embeddings for collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare user and item interaction data for both train and test sets\n",
    "train_user_ids = train_data['uid_review'].astype('category').cat.codes.values\n",
    "train_item_ids = train_data['anime_uid'].astype('category').cat.codes.values\n",
    "test_user_ids = test_data['uid_review'].astype('category').cat.codes.values\n",
    "test_item_ids = test_data['anime_uid'].astype('category').cat.codes.values\n",
    "\n",
    "num_users = data_with_emotions['uid_review'].nunique()\n",
    "num_items = data_with_emotions['anime_uid'].nunique()\n",
    "embedding_size = 20\n",
    "\n",
    "# Neural Collaborative Filtering model architecture\n",
    "user_input = Input(shape=(1,))\n",
    "item_input = Input(shape=(1,))\n",
    "content_input = Input(shape=(train_content_features_dense.shape[1],))\n",
    "\n",
    "user_embedding = Embedding(num_users, embedding_size, input_length=1)(user_input)\n",
    "item_embedding = Embedding(num_items, embedding_size, input_length=1)(item_input)\n",
    "user_vec = Flatten()(user_embedding)\n",
    "item_vec = Flatten()(item_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Model Architecture\n",
    "- Combine content-based and collaborative filtering features in a neural network\n",
    "\n",
    "- Evaluate model with MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = concatenate([user_vec, item_vec, content_input])\n",
    "dense_layer_1 = Dense(128, activation='relu')(concatenated)\n",
    "output = Dense(1, activation='linear')(dense_layer_1)\n",
    "\n",
    "model_hybrid = Model(inputs=[user_input, item_input, content_input], outputs=output)\n",
    "model_hybrid.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "\n",
    "# Set a smaller batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Train the Model\n",
    "model_hybrid.fit(\n",
    "    [train_user_ids, train_item_ids, train_content_features_dense],\n",
    "    train_data['Overall'].values, \n",
    "    batch_size=batch_size, \n",
    "    epochs=5, \n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Evaluate the Model using MSE\n",
    "mse_loss = model_hybrid.evaluate(\n",
    "    [test_user_ids, test_item_ids, test_content_features_dense], \n",
    "    test_data['Overall'].values\n",
    ")\n",
    "print(f\"Mean Squared Error on Test Data: {mse_loss}\")\n",
    "\n",
    "# Predict on the test data\n",
    "test_predictions = model_hybrid.predict(\n",
    "    [test_user_ids, test_item_ids, test_content_features_dense]\n",
    ")\n",
    "\n",
    "# Calculate MAE\n",
    "mae_metric = MeanAbsoluteError()\n",
    "mae_metric.update_state(test_data['Overall'].values, test_predictions)\n",
    "mae = mae_metric.result().numpy()\n",
    "print(f\"Mean Absolute Error on Test Data: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF: Adjusted with parameters max_features and ngram_range.\n",
    "- Regularization: Added L1 and L2 regularization to the first dense layer.\n",
    "- Cross-Validation: Implemented with 5-fold cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in 'synopsis' with empty string\n",
    "data_with_emotions['synopsis'].fillna('', inplace=True)\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = train_test_split(data_with_emotions, test_size=0.2, random_state=13)\n",
    "\n",
    "# Apply TF-IDF to the training set and transform the test set\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(1, 2))\n",
    "train_content_features = tfidf.fit_transform(train_data['synopsis'])\n",
    "test_content_features = tfidf.transform(test_data['synopsis'])\n",
    "\n",
    "# Convert sparse TF-IDF matrix to a dense format for both train and test sets\n",
    "train_content_features_dense = train_content_features.todense()\n",
    "test_content_features_dense = test_content_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare user and item interaction data for both train and test sets\n",
    "train_user_ids = train_data['uid_review'].astype('category').cat.codes.values\n",
    "train_item_ids = train_data['anime_uid'].astype('category').cat.codes.values\n",
    "test_user_ids = test_data['uid_review'].astype('category').cat.codes.values\n",
    "test_item_ids = test_data['anime_uid'].astype('category').cat.codes.values\n",
    "\n",
    "num_users = data_with_emotions['uid_review'].nunique()\n",
    "num_items = data_with_emotions['anime_uid'].nunique()\n",
    "embedding_size = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create the model\n",
    "def create_model():\n",
    "    user_input = Input(shape=(1,))\n",
    "    item_input = Input(shape=(1,))\n",
    "    content_input = Input(shape=(train_content_features_dense.shape[1],))\n",
    "\n",
    "    user_embedding = Embedding(num_users, embedding_size, input_length=1)(user_input)\n",
    "    item_embedding = Embedding(num_items, embedding_size, input_length=1)(item_input)\n",
    "    user_vec = Flatten()(user_embedding)\n",
    "    item_vec = Flatten()(item_embedding)\n",
    "\n",
    "    concatenated = concatenate([user_vec, item_vec, content_input])\n",
    "    dense_layer_1 = Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(concatenated)\n",
    "    dropout_1 = Dropout(0.3)(dense_layer_1)\n",
    "    dense_layer_2 = Dense(128, activation='relu')(dropout_1)\n",
    "    output = Dense(1, activation='linear')(dense_layer_2)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input, content_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Tuned Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-Fold Cross Validator\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "\n",
    "# Convert features and labels to numpy arrays for indexing\n",
    "train_user_ids = np.array(train_user_ids)\n",
    "train_item_ids = np.array(train_item_ids)\n",
    "train_content_features_dense = np.array(train_content_features_dense)\n",
    "train_labels = np.array(train_data['Overall'].values)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "mae_scores=[]\n",
    "for train_indices, test_indices in kfold.split(train_user_ids):\n",
    "    # Split data\n",
    "    X_train_user, X_test_user = train_user_ids[train_indices], train_user_ids[test_indices]\n",
    "    X_train_item, X_test_item = train_item_ids[train_indices], train_item_ids[test_indices]\n",
    "    X_train_content, X_test_content = train_content_features_dense[train_indices], train_content_features_dense[test_indices]\n",
    "    y_train, y_test = train_labels[train_indices], train_labels[test_indices]\n",
    "\n",
    "    # Create a new model instance\n",
    "    model = create_model()\n",
    "    \n",
    "    # Fit data to model\n",
    "    model.fit(\n",
    "        [X_train_user, X_train_item, X_train_content],\n",
    "        y_train, \n",
    "        batch_size=32, \n",
    "        epochs=10, \n",
    "        validation_split=0.1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    scores = model.evaluate(\n",
    "        [X_test_user, X_test_item, X_test_content],\n",
    "        y_test\n",
    "    )\n",
    "\n",
    "print(f'MAE for fold {fold_no}: {scores[1]}')\n",
    "mae_scores.append(scores[1])\n",
    "fold_no += 1\n",
    "\n",
    "# Calculate and print the average MAE across all folds\n",
    "average_mae = sum(mae_scores) / len(mae_scores)\n",
    "print(f'Average MAE across all folds: {average_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Tuned Model with Age & Gender Features\n",
    "Neural network-based hybrid recommendation system model predicts how a user might rate an anime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Input, concatenate, Dense\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "# Fill NaN values in 'synopsis' with empty string\n",
    "data_with_emotions['synopsis'].fillna('', inplace=True)\n",
    "\n",
    "# Define text preprocessing pipeline\n",
    "text_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(1, 2)))\n",
    "])\n",
    "\n",
    "# Define numerical feature preprocessing\n",
    "numerical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define overall preprocessor\n",
    "hybrid_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_pipeline, 'synopsis'),\n",
    "        ('age', numerical_transformer, ['age']),\n",
    "        ('gender', OneHotEncoder(), ['gender'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = train_test_split(data_with_emotions, test_size=0.2, random_state=13)\n",
    "train_transformed = hybrid_preprocessor.fit_transform(train_data)\n",
    "test_transformed = hybrid_preprocessor.transform(test_data)\n",
    "\n",
    "# Convert to dense array if it's a sparse matrix\n",
    "if scipy.sparse.issparse(train_transformed):\n",
    "    train_transformed = train_transformed.toarray()\n",
    "if scipy.sparse.issparse(test_transformed):\n",
    "    test_transformed = test_transformed.toarray()\n",
    "\n",
    "# Prepare user and item IDs\n",
    "train_user_ids = train_data['uid_review'].astype('category').cat.codes.values\n",
    "train_item_ids = train_data['anime_uid'].astype('category').cat.codes.values\n",
    "test_user_ids = test_data['uid_review'].astype('category').cat.codes.values\n",
    "test_item_ids = test_data['anime_uid'].astype('category').cat.codes.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Hybrid Regression Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Save the preprocessor\n",
    "joblib.dump(hybrid_preprocessor, 'hybrid_preprocessor.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Saving with pickle\n",
    "with open('hybrid_preprocessor.pkl', 'wb') as file:\n",
    "    pickle.dump(hybrid_preprocessor, file)\n",
    "\n",
    "# Loading with pickle\n",
    "with open('hybrid_preprocessor.pkl', 'rb') as file:\n",
    "    hybrid_preprocessor = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "def create_model(num_users, num_items, input_shape):\n",
    "    user_input = Input(shape=(1,))\n",
    "    item_input = Input(shape=(1,))\n",
    "    additional_input = Input(shape=(input_shape,))\n",
    "\n",
    "    # Make sure `input_dim` is set correctly\n",
    "    user_embedding = Embedding(input_dim=num_users + 1, output_dim=20, input_length=1)(user_input)\n",
    "    item_embedding = Embedding(input_dim=num_items + 1, output_dim=20, input_length=1)(item_input)\n",
    "    user_vec = Flatten()(user_embedding)\n",
    "    item_vec = Flatten()(item_embedding)\n",
    "\n",
    "    concatenated = concatenate([user_vec, item_vec, additional_input])\n",
    "    dense_layer = Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(concatenated)\n",
    "    output = Dense(1, activation='linear')(dense_layer)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input, additional_input], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(0.0005), loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "mae_scores = []\n",
    "\n",
    "for train_indices, test_indices in kfold.split(train_transformed):\n",
    "    X_train, X_test = train_transformed[train_indices], train_transformed[test_indices]\n",
    "    y_train, y_test = train_data.iloc[train_indices]['Overall'], train_data.iloc[test_indices]['Overall']\n",
    "    train_user_ids_fold, test_user_ids_fold = train_user_ids[train_indices], train_user_ids[test_indices]\n",
    "    train_item_ids_fold, test_item_ids_fold = train_item_ids[train_indices], train_item_ids[test_indices]\n",
    "\n",
    "    # Create the model\n",
    "    rating_predictor = create_model(np.max(train_user_ids_fold) + 1, np.max(train_item_ids_fold) + 1, X_train.shape[1])\n",
    "\n",
    "    # Train the model\n",
    "    history = rating_predictor.fit(\n",
    "        [train_user_ids_fold, train_item_ids_fold, X_train],\n",
    "        y_train,\n",
    "        batch_size=32,\n",
    "        epochs=10\n",
    "    )\n",
    "\n",
    "    # Predict and evaluate\n",
    "    predictions = rating_predictor.predict([test_user_ids_fold, test_item_ids_fold, X_test])\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    mae_scores.append(mae)\n",
    "    print(f'Fold MAE: {mae}')\n",
    "\n",
    "# Average MAE across all folds\n",
    "average_mae = np.mean(mae_scores)\n",
    "print(f'Average Mean Absolute Error across all folds: {average_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Hybrid Recommendation System Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "joblib.dump(rating_predictor, 'saved_models/rating_predictor.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving with pickle\n",
    "with open('saved_models/rating_predictor.pkl', 'wb') as file:\n",
    "    pickle.dump(rating_predictor, file)\n",
    "\n",
    "# Loading with pickle\n",
    "with open('saved_models/rating_predictor.pkl', 'rb') as file:\n",
    "    rating_predictor = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "### Model Performance\n",
    "- The average MAE across all folds is approx 1.1120. MAE of around 1.1120 indicates that this model predicts user Overall ratings, on average, within 1 rating point from the true value.\n",
    "\n",
    "- Model's performance is relatively stable, indicated by the consistent MAE values for individual folds. The model is not highly dependent on specific segments of the data.\n",
    "\n",
    "### Training Progress\n",
    "- The training loss decreases consistently across epochs for each fold, starting from a high value (around 22-23) and reducing to less than 1 by the 10th epoch. This indicates that the model is learning from the training data and improving its predictive accuracy over time.\n",
    "\n",
    "### Features Used in Model\n",
    "- The model includes embeddings for users and items. These embeddings are critical in recommendation systems as they help in capturing the latent factors and interactions between users and items.\n",
    "\n",
    "- The model also incorporates Age & Gender processed by the hybrid_preprocessor, possibly capturing demographic aspects related to users or items.\n",
    "\n",
    "### Conclusion\n",
    "The model shows consistent learning and a reasonable accuracy in predicting Overall ratings. However, must consider overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; border: 2px solid black;\">\n",
    "    <!-- Add padding to the div element -->\n",
    "    <div style=\"padding: 20px 0;\">\n",
    "        <img src=\"images/K_G_Logo.png\" alt=\"K&G Logo\" width=\"150\" height=\"150\">\n",
    "    </div>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <font size=\"1\">Image by DALL-E</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-interface Application\n",
    "- See [MoodyManga](https://moody-manga-app-ginakari.streamlit.app/) for public-facing interactive app\n",
    "- See Moody_Manga_App --> 'moody_manga_app.py' for completed app details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Recommendations Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, concatenate, Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load pre-saved models and preprocessors\n",
    "mood_classifier = load('saved_models/mood_classifier.joblib')\n",
    "rfc_preprocessor = load('saved_models/rfc_preprocessor.joblib')\n",
    "hybrid_preprocessor = load('saved_models/hybrid_preprocessor.joblib')\n",
    "\n",
    "# Ensure 'gender' is categorical\n",
    "data_with_emotions['gender'] = data_with_emotions['gender'].astype('category')\n",
    "\n",
    "def sort_anime_based_on_predicted_ratings(data_with_emotions, user_age, user_gender, rating_predictor, hybrid_preprocessor):\n",
    "    # Prepare additional input data\n",
    "    additional_input_data = pd.DataFrame({\n",
    "        'synopsis': data_with_emotions['synopsis'],\n",
    "        'age': [user_age] * len(data_with_emotions),\n",
    "        'gender': [user_gender] * len(data_with_emotions)\n",
    "    })\n",
    "\n",
    "    # Preprocess the additional input data\n",
    "    additional_input_transformed = hybrid_preprocessor.transform(additional_input_data).todense()\n",
    "\n",
    "    # Extract user and item IDs from the anime data\n",
    "    user_ids = data_with_emotions['uid_review'].astype('category').cat.codes.values\n",
    "    item_ids = data_with_emotions['anime_uid'].astype('category').cat.codes.values\n",
    "\n",
    "    # Predict ratings using the rating predictor model\n",
    "    predicted_ratings = rating_predictor.predict([user_ids, item_ids, additional_input_transformed])\n",
    "\n",
    "    # Add the predicted ratings to the anime data\n",
    "    data_with_emotions['predicted_rating'] = predicted_ratings\n",
    "\n",
    "    # Sort the anime based on predicted ratings\n",
    "    sorted_anime = data_with_emotions.sort_values(by='predicted_rating', ascending=False)\n",
    "\n",
    "    return sorted_anime\n",
    "\n",
    "\n",
    "def generate_playlist(mood, age, gender, data_with_emotions, mood_classifier, rating_predictor, rfc_preprocessor, hybrid_preprocessor):\n",
    "    # Hardcoded mood-to-genre mapping\n",
    "    mood_to_genre_mapping = {\n",
    "        'Happy': ['Slice of Life', 'Comedy', 'Supernatural', 'Drama', 'Romance', 'Shoujo'],\n",
    "        'Sad': ['Action', 'Drama', 'Military'],\n",
    "        'Excited': ['Mystery', 'Police', 'Psychological', 'Supernatural', 'Thriller', 'Shounen'],\n",
    "        'Angry': ['Action', 'Horror', 'Demons', 'Drama', 'Vampire', 'Shoujo'],\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "\n",
    "    # Predict the mood for each anime\n",
    "    features = data_with_emotions[['genre', 'popularity']]\n",
    "    features_transformed = rfc_preprocessor.transform(features)\n",
    "    data_with_emotions['predicted_mood'] = mood_classifier.predict(features_transformed)\n",
    "\n",
    "    # Implement mood-based filtering\n",
    "    genres_based_on_mood = mood_to_genre_mapping.get(mood, ['Default Genre'])\n",
    "    average_popularity = data_with_emotions[data_with_emotions['genre'].isin(genres_based_on_mood)]['popularity'].mean()\n",
    "    genres_str = ', '.join(genres_based_on_mood)\n",
    "    mood_input_df = pd.DataFrame({'genre': [genres_str], 'popularity': [average_popularity]})\n",
    "    mood_input_transformed = rfc_preprocessor.transform(mood_input_df)\n",
    "    predicted_mood_category = mood_classifier.predict(mood_input_transformed)[0]\n",
    "    filtered_anime = data_with_emotions[data_with_emotions['predicted_mood'] == predicted_mood_category]\n",
    "\n",
    "    # Implement rating-based sorting\n",
    "    sorted_anime = sort_anime_based_on_predicted_ratings(filtered_anime, age, gender, rating_predictor, hybrid_preprocessor)\n",
    "\n",
    "    return sorted_anime.head(10)\n",
    "\n",
    "# User Inputs - Replace with Python input() calls or hardcoded values for testing\n",
    "user_mood = input('Select your mood (Happy, Sad, Excited, Angry, Fear, Neutral): ')\n",
    "user_age = int(input('Enter your age (0-100): '))\n",
    "user_gender = input('Select your gender (Male, Female, Non-Binary): ')\n",
    "\n",
    "# Generate Playlist\n",
    "playlist = generate_playlist(user_mood, user_age, user_gender, data_with_emotions, mood_classifier, rating_predictor, rfc_preprocessor, hybrid_preprocessor)\n",
    "\n",
    "# Display Playlist\n",
    "print(playlist[['title', 'synopsis', 'genre', 'predicted_mood', 'predicted_rating']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; border: 2px solid black;\">\n",
    "    <!-- Add padding to the div element -->\n",
    "    <div style=\"padding: 20px 0;\">\n",
    "        <img src=\"images/K_G_Logo.png\" alt=\"K&G Logo\" width=\"150\" height=\"150\">\n",
    "    </div>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <font size=\"1\">Image by DALL-E</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **More Emotions:** Train model on a larger selection of moods\n",
    "\n",
    "- **Mood Mapping:** Expand genre-to-mood labeling\n",
    "\n",
    "- **User Feedback:** Real-time user feedback to improve predictions and app functions\n",
    "\n",
    "- **Image Classification:** Use img_url to process image data with Convolution Neural Networks to use for more advanced mood predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; border: 2px solid black;\">\n",
    "    <!-- Add padding to the div element -->\n",
    "    <div style=\"padding: 20px 0;\">\n",
    "        <img src=\"images/K_G_Logo.png\" alt=\"K&G Logo\" width=\"150\" height=\"150\">\n",
    "    </div>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <font size=\"1\">Image by DALL-E</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyAnimeList:\n",
    "- [Animes](https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews/?select=animes.csv)\n",
    "\n",
    "- [Profiles](https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews/?select=profiles.csv)\n",
    "\n",
    "- [Reviews](https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews/?select=reviews.csv)\n",
    "\n",
    "Related Referenced Sources:\n",
    "- [azathoth42/myanimelist](https://www.kaggle.com/datasets/azathoth42/myanimelist)\n",
    "\n",
    "- [CooperUnion/anime-recommendations-database](https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database)\n",
    "\n",
    "- [natlee/myanimelist-comment-dataset](https://www.kaggle.com/datasets/natlee/myanimelist-comment-dataset)\n",
    "\n",
    "- [MyAnimeList](https://myanimelist.net/)\n",
    "\n",
    "### TMDB:\n",
    "- [Anime](https://www.themoviedb.org/list/7102-anime)\n",
    "\n",
    "### Top Anime\n",
    "- [Top 10000 Anime Movies, OVA's and Tv-Shows](https://www.kaggle.com/datasets/thomaskonstantin/top-10000-anime-movies-ovas-and-tvshows)\n",
    "\n",
    "### Outside Data:\n",
    "- [NotifyVisitors](https://notifyvisitors.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was, of course, developed using knowledge from ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; border: 2px solid black;\">\n",
    "    <!-- Add padding to the div element -->\n",
    "    <div style=\"padding: 20px 0;\">\n",
    "        <img src=\"images/K_G_Logo.png\" alt=\"K&G Logo\" width=\"150\" height=\"150\">\n",
    "    </div>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <font size=\"1\">Image by DALL-E</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Us\n",
    "**Kari Primiano**\n",
    "\n",
    "- Tech Lead\n",
    "- Email: kkprim@gmail.com\n",
    "- github.com/kkprim\n",
    "\n",
    "**Gina Guerin**\n",
    "\n",
    "- Github & Presentation Lead\n",
    "- Email: gina.b.guerin@gmail.com\n",
    "- github.com/ginaguerin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; border: 2px solid black;\">\n",
    "    <!-- Add padding to the div element -->\n",
    "    <div style=\"padding: 20px 0;\">\n",
    "        <img src=\"images/K_G_Logo.png\" alt=\"K&G Logo\" width=\"150\" height=\"150\">\n",
    "    </div>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "    <font size=\"1\">Image by DALL-E</font>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
